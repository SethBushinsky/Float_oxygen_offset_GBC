{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in crossovers, calculate group statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# import cartopy.crs as ccrs\n",
    "# import cartopy.feature as cfeature\n",
    "from scipy import stats\n",
    "# import carbon_utils\n",
    "# import PyCO2SYS as pyco2\n",
    "from tabulate import tabulate\n",
    "from io import StringIO  # for creating a string buffer to store the table\n",
    "from multiprocessing import Pool\n",
    "import functions.pressure_level_glodap_mean as pl\n",
    "# from datetime import datetime\n",
    "import functions.plot_offsets_crossover_plot_only as plot_gdap\n",
    "import time\n",
    "import functions.outlier_filter_ESD_test as outlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjustment=False\n",
    "# glodap_offsets_filename = 'glodap_offsets_100km_1450_to_2000_100m_0.005dens_0.005spice_4.nc'\n",
    "# glodap_offsets_filename = 'glodap_offsets_100km_2_to_50_50m_0.1dens_0.1spice_5.nc'\n",
    "# read in a user-created text file to point to local directories to avoid having to change this every time \n",
    "# we update code\n",
    "lines=[]\n",
    "with open('path_file.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "count = 0\n",
    "for line in lines:\n",
    "    count += 1\n",
    "    index = line.find(\"=\")\n",
    "    #print(f'line {count}: {line}')\n",
    "    #print(index)\n",
    "    #print(line[0:index])\n",
    "    line = line.rstrip()\n",
    "    if line[0:index].find(\"argo\")>=0:\n",
    "        argo_path=line[index+1:]\n",
    "    elif line[0:index].find(\"liar\")>=0:\n",
    "        liar_dir=line[index+1:]\n",
    "    elif line[0:index].find(\"matlab\")>=0:\n",
    "        matlab_dir=line[index+1:]\n",
    "\n",
    "if adjustment is True:\n",
    "    argo_path = argo_path + '../Corrected/Sprof/'\n",
    "\n",
    "# Set the paths\n",
    "# for deep o2 manuscript:\n",
    "output_dir = argo_path + '../output_100km_1400_to_2100_100m_0.005dens_0.005spice_ESPER_14/'\n",
    "# output_dir = argo_path + '../output_100km_1400_to_2100_100m_0.005dens_0.005spice_LIPHR_14/'\n",
    "\n",
    "# output_dir = argo_path + '../output_50km_1_to_50_25m_0.05dens_0.05spice_ESPER_14/'\n",
    "output_dir = argo_path + '../output_100km_1400_to_2100_100m_0.005dens_0.005spice_ESPER_14/'\n",
    "# output_dir = argo_path + '../output_100km_400_to_2100_100m_0.005dens_0.005spice_ESPER_14/'\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Check for a glodap_offsets_plots directory, create if it does not exist\n",
    "offset_dir = output_dir + 'glodap_offset_plots/'\n",
    "if not os.path.isdir(offset_dir):\n",
    "    os.mkdir(offset_dir)\n",
    "# if not os.path.isdir(grouped_plot_dir):\n",
    "#     os.mkdir(grouped_plot_dir)\n",
    "\n",
    "\n",
    "# inputs that will not normally change\n",
    "var_list_plot = ['PRES_ADJUSTED','TEMP_ADJUSTED','PSAL_ADJUSTED','DOXY_ADJUSTED','NITRATE_ADJUSTED',\n",
    "                    'DIC','pH_25C_TOTAL_ADJUSTED','PH_IN_SITU_TOTAL_ADJUSTED','PDENS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify files, pressure levels, float age bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user inputs\n",
    "# calculate trimmed means at different pressure levels for listed files\n",
    "grouped_plot_dir = offset_dir + '2024_10_22/'\n",
    "# grouped_plot_dir = offset_dir + '2024_09_25/'\n",
    "\n",
    "# todays_date = datetime.today().strftime('%Y_%m_%d')\n",
    "# grouped_plot_dir = offset_dir + 'grouped_plots/' + todays_date + '/'\n",
    "\n",
    "# grouped_plot_dir = offset_dir + 'grouped_plots/levels_mean_saved/'\n",
    "\n",
    "if not os.path.isdir(grouped_plot_dir):\n",
    "    os.mkdir(grouped_plot_dir)\n",
    "\n",
    "# for manuscript:\n",
    "glodap_offsets_filenames = ['glodap_offsets_100km_1400_to_2100_100m_0.005dens_0.005spice_ESPER_14.nc']\n",
    "\n",
    "# glodap_offsets_filenames = ['glodap_offsets_100km_1400_to_2100_100m_0.005dens_0.005_spice_7.nc']\n",
    "# glodap_offsets_filenames = ['glodap_offsets_100km_1_to_550_50m_0.05dens_0.05spice_6.nc', 'glodap_offsets_100km_400_to_2100_100m_0.005dens_0.005spice_6.nc']\n",
    "# glodap_offsets_filenames = ['glodap_offsets_50km_1_to_550_50m_0.05dens_0.05_spice_7.nc', 'glodap_offsets_100km_400_to_2100_100m_0.005dens_0.005_spice_7.nc']\n",
    "# glodap_offsets_filenames = ['glodap_offsets_100km_400_to_2100_100m_0.005dens_0.005spice_LIPHR_7.nc']\n",
    "# glodap_offsets_filenames = ['output_50km_1_to_550_50m_0.05dens_0.05spice_ESPER_14.nc', 'glodap_offsets_100km_1400_to_2100_100m_0.005dens_0.005spice_ESPER_14.nc']\n",
    "# glodap_offsets_filenames = ['glodap_offsets_50km_1_to_550_50m_0.05dens_0.05spice_ESPER_14.nc','glodap_offsets_100km_400_to_2100_100m_0.005dens_0.005spice_ESPER_14.nc']\n",
    "# glodap_offsets_filenames = ['glodap_offsets_100km_1400_to_2100_100m_0.005dens_0.005spice_ESPER_14.nc']\n",
    "# glodap_offsets_filenames = ['glodap_offsets_50km_1_to_50_25m_0.05dens_0.05spice_ESPER_14.nc']\n",
    "# glodap_offsets_filenames = ['glodap_offsets_50km_1_to_500_25m_0.05dens_0.05spice_ESPER_14.nc','glodap_offsets_100km_400_to_2100_100m_0.005dens_0.005spice_ESPER_14.nc']\n",
    "\n",
    "pressure_levels = [1500, 2000]  # Adjust as needed\n",
    "# pressure_levels = [0, 100, 200, 300, 400, 500, 750, 1000, 1250, 1500, 2000]  # Adjust as needed\n",
    "# pressure_levels = [1500, 2000]  # Adjust as needed\n",
    "# pressure_levels = [1, 50]  # Adjust as needed\n",
    "\n",
    "# pressure_levels = [1500, 1700, 2000]  # Adjust as needed\n",
    "\n",
    "year_filt = 0\n",
    "year_plus_minus = 5\n",
    "\n",
    "# bin edges in days to calculat mean offset as it evolves over the float/optode's age\n",
    "# float_age_bins = [0, 365/2, 365, 365*2, 365*3, 365*7]\n",
    "# float_age_bins = [0, 365, 365*7]\n",
    "float_age_bins = [0, 365*10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the list of arguments to pass to pressure_level_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glodap_offsets = []\n",
    "delay_between_starts = 5  # number of seconds for each additional instance to wait, may help with a memory bottleneck \n",
    "\n",
    "for filename in glodap_offsets_filenames:\n",
    "        ds = xr.load_dataset(argo_path + '../output_' + filename[15:-3] + '/' +filename)\n",
    "        # ds = xr.load_dataset(output_dir+filename)\n",
    "        glodap_offsets.append(ds)\n",
    "\n",
    "gdap_mean_args = []\n",
    "for idx, gdap_offsets_n in enumerate(glodap_offsets):\n",
    "        time_delay = 0\n",
    "        gdap_offsets_n_temp = gdap_offsets_n.copy()\n",
    "        for j in range(len(pressure_levels) - 1):\n",
    "                # only add to the processing list if the pressure level is in the correct range for a given file, otherwise skip \n",
    "                if np.logical_and(np.nanmin(gdap_offsets_n_temp.PRES_ADJUSTED_float)<= pressure_levels[j+1], np.nanmax(gdap_offsets_n_temp.PRES_ADJUSTED_float)>= pressure_levels[j]):\n",
    "                        print(idx)\n",
    "                        print(pressure_levels[j])\n",
    "                        time_delay = time_delay+ delay_between_starts\n",
    "                        gdap_mean_args_n = [(argo_path,grouped_plot_dir,\n",
    "                                glodap_offsets_filenames[idx][0:-3],\n",
    "                                gdap_offsets_n_temp, \n",
    "                                var_list_plot,year_filt,pressure_levels[j],\n",
    "                                pressure_levels[j+1],year_plus_minus,time_delay, float_age_bins) ]\n",
    "                        gdap_mean_args.extend(gdap_mean_args_n)\n",
    "                # break   \n",
    "        # break\n",
    "print(len(gdap_mean_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run pressure level filter for all files, all pressures - calculates mean float crossovers in user-defined pressure ranges, age bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, this can be skipped if already run\n",
    "num_processes = 18 # number of cpus to use \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        # Use pool.starmap with the list of arguments\n",
    "        pool.starmap(pl.pressure_level_filter, gdap_mean_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Take netcdf output of crossovers and load all into gdap_offsets and mean_gdap_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all .nc files into grouped arrays \n",
    "\n",
    "\n",
    "mean_gdap_offsets = {}\n",
    "gdap_offsets = {}\n",
    "# load .nc files saved out for pressure levels\n",
    "for idx, filename in enumerate(glodap_offsets_filenames):\n",
    "\n",
    "        trimmed_means = {}\n",
    "        # variables with offsets that you want to trim\n",
    "\n",
    "        gdap_offsets_file = {}\n",
    "        for j in range(len(pressure_levels) - 1):\n",
    "                pressure_level_min = pressure_levels[j]\n",
    "                nc_filename_mean = grouped_plot_dir+filename[0:-3]+ '_floatmean_withcalibration_depth_grouped_' + \\\n",
    "                                        'year_filt_' + str(year_filt) +'_' + str(year_plus_minus) + '_level_' + str(pressure_level_min) + '.nc'\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                        ds = xr.load_dataset(nc_filename_mean)\n",
    "                except:\n",
    "                        continue\n",
    "                trimmed_means[f'level_{pressure_levels[j]}'] = ds\n",
    "                \n",
    "                nc_filename_all = grouped_plot_dir+filename[0:-3]+ '_all_offsets_depth_grouped_' + \\\n",
    "                                        'year_filt_' + str(year_filt) +'_' + str(year_plus_minus) + '_level_' + str(pressure_level_min) + '.nc'\n",
    "                all_offsets_ds = xr.load_dataset(nc_filename_all)\n",
    "                gdap_offsets_file[f'level_{pressure_levels[j]}'] = all_offsets_ds\n",
    "                # break\n",
    "        mean_gdap_offsets[glodap_offsets_filenames[idx]]=trimmed_means\n",
    "        gdap_offsets[glodap_offsets_filenames[idx]]=gdap_offsets_file\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot individual crossover plots - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For making individual plots\n",
    "from importlib import reload\n",
    "\n",
    "parallel_on = True\n",
    "reload(plot_gdap)\n",
    "num_processes = 16 # number of cpus to use \n",
    "\n",
    "# make individual glodap crossover plots for each pressure level\n",
    "# filename = 'glodap_offsets_100km_1400_to_2100_100m_0.005dens_0.005_spice_7.nc'\n",
    "for idx, filename in enumerate(glodap_offsets_filenames):\n",
    "    for j in range(len(pressure_levels) - 1):\n",
    "        pressure_level_min = pressure_levels[j]\n",
    "        pressure_level_max = pressure_levels[j+1]\n",
    "\n",
    "        # single_level_offsets = gdap_offsets[filename]['level_1500']\n",
    "        try:\n",
    "            single_level_offsets = gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "            mean_level_offsets = mean_gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "        individual_plot_dir = grouped_plot_dir+filename[0:-3]+ '_all_offsets_depth_grouped_' + \\\n",
    "                                                'year_filt_' + str(year_filt) +'_' + str(year_plus_minus) + \\\n",
    "                                                    '_level_' + str(pressure_level_min) + '_individual_floats/'\n",
    "        if not os.path.isdir(individual_plot_dir):\n",
    "            os.mkdir(individual_plot_dir)\n",
    "            \n",
    "        offsets_g = single_level_offsets.groupby(single_level_offsets.main_float_wmo)\n",
    "        print(individual_plot_dir)\n",
    "        \n",
    "        if parallel_on is False:\n",
    "            if 'checklist' not in locals():\n",
    "                unique_labels = list(offsets_g.groups.keys())\n",
    "                checklist = [False] * len(unique_labels)\n",
    "\n",
    "            print(unique_labels)\n",
    "        \n",
    "            for n, g in offsets_g:\n",
    "                \n",
    "                index_to_check = unique_labels.index(n)\n",
    "\n",
    "                # print(checklist[index_to_check])\n",
    "                if not checklist[index_to_check]:\n",
    "                    plot_gdap.plot_glodap_crossovers(individual_plot_dir,\n",
    "                                mean_level_offsets,\n",
    "                                g, [pressure_level_min, pressure_level_max], float_age_bins)\n",
    "                    \n",
    "                    # Find the index of the label in unique_labels\n",
    "                    index_to_mark_off = unique_labels.index(n)\n",
    "                    \n",
    "                    # Mark off the corresponding position in the checklist\n",
    "                    checklist[index_to_mark_off] = True\n",
    "                    break\n",
    "        else:\n",
    "            if __name__ == \"__main__\":\n",
    "                \n",
    "                with Pool(processes=num_processes) as pool:\n",
    "                    # Create a list of arguments for pool.starmap\n",
    "                    gdap_cross_plot_args = [(individual_plot_dir,\n",
    "                            mean_level_offsets,\n",
    "                            g, [pressure_level_min, pressure_level_max], float_age_bins) for n, g in offsets_g]\n",
    "                    # print('Here')\n",
    "                    # Use pool.starmap with the list of arguments\n",
    "                    pool.starmap(plot_gdap.plot_glodap_crossovers, gdap_cross_plot_args)\n",
    "        # break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Code below for testing, some output statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### test cell for pressure_level_glodap_mean\n",
    "\n",
    "pressure_level_min = pressure_levels[j]\n",
    "pressure_level_max = pressure_levels[j+1]\n",
    "\n",
    "time.sleep(time_delay) \n",
    "# set all data not at that pressure level to nan                  \n",
    "for var in var_list_plot:\n",
    "    pressure_index = np.logical_and(gdap_offsets_n_temp['PRES_ADJUSTED_float']>pressure_level_min, \n",
    "                            gdap_offsets_n_temp['PRES_ADJUSTED_float']<=pressure_level_max)\n",
    "\n",
    "    if year_filt==0:\n",
    "        gdap_offsets_n_temp[var + '_float'] = gdap_offsets_n_temp[var + '_float'].where(pressure_index)\n",
    "        gdap_offsets_n_temp[var + '_glodap'] = gdap_offsets_n_temp[var + '_glodap'].where(pressure_index)\n",
    "        gdap_offsets_n_temp[var + '_offset'] = gdap_offsets_n_temp[var + '_offset'].where(pressure_index)\n",
    "    elif year_filt==1:\n",
    "        year_index = np.abs(gdap_offsets_n_temp.main_float_juld.dt.year-gdap_offsets_n_temp.glodap_datetime.dt.year)<=year_plus_minus\n",
    "        gdap_offsets_n_temp[var + '_float'] = \\\n",
    "            gdap_offsets_n_temp[var + '_float'].where(np.logical_and(pressure_index, year_index))\n",
    "        gdap_offsets_n_temp[var + '_glodap'] = \\\n",
    "            gdap_offsets_n_temp[var + '_glodap'].where(np.logical_and(pressure_index, year_index))\n",
    "        gdap_offsets_n_temp[var + '_offset'] = \\\n",
    "            gdap_offsets_n_temp[var + '_offset'].where(np.logical_and(pressure_index, year_index))\n",
    "        \n",
    "# then group by wmo and proceed with DOXY_trimmed calculations\n",
    "offsets_g = gdap_offsets_n_temp.groupby(gdap_offsets_n_temp.main_float_wmo)\n",
    "\n",
    "if pressure_level_min<=400:\n",
    "    # adding option to filter by time of year as well - for use in surface data\n",
    "    time_filt = 1\n",
    "    filt_days = 10\n",
    "else:\n",
    "    time_filt=0\n",
    "\n",
    "\n",
    "# DOXY_ADJUSTED_offset_trimmed = []\n",
    "# DOXY_ADJUSTED_offset_trimmed = []\n",
    "# Create an empty data variable for DOXY offset trimmed with the same dimensions as N_CROSSOVERS\n",
    "empty_data = np.empty(len(gdap_offsets_n_temp['N_CROSSOVERS']))\n",
    "empty_data[:] = np.nan\n",
    "\n",
    "\n",
    "# Create a new xarray DataArray with the empty data and the same coordinates\n",
    "new_data_array = xr.DataArray(empty_data, coords={'N_CROSSOVERS': gdap_offsets_n_temp['N_CROSSOVERS']}, dims=['N_CROSSOVERS'])\n",
    "gdap_offsets_n_temp['DOXY_ADJUSTED_offset_trimmed'] = new_data_array\n",
    "\n",
    "for n,g in offsets_g:\n",
    "\n",
    "    # run a GESD test using \"test_num\" number of possible outliers\n",
    "    test_num = int((len(g.DOXY_ADJUSTED_offset.dropna(dim=\"N_CROSSOVERS\", how=\"any\").values)*.1)) # allowing for ~10 % to be outliers\n",
    "    ESD_test_out = outlier.ESD_Test(g.DOXY_ADJUSTED_offset.dropna(dim=\"N_CROSSOVERS\", how=\"any\").values, 0.05, test_num, False, True)\n",
    "\n",
    "    # only trim the data if deep, otherwise apply a day of year test but no other filtering \n",
    "    if time_filt==1:\n",
    "        within_days = np.logical_or(np.abs(g.main_float_juld.dt.dayofyear - g.glodap_datetime.dt.dayofyear)<=filt_days, \n",
    "                        np.abs(g.main_float_juld.dt.dayofyear - g.glodap_datetime.dt.dayofyear)>(365-filt_days)) \n",
    "        temp_o2_offset = g.DOXY_ADJUSTED_offset.where(within_days)\n",
    "    else:         # create temp_o2_offest to set all datapoints to nans that the GESD test says are outliers\n",
    "\n",
    "        temp_o2_offset = g.DOXY_ADJUSTED_offset\n",
    "        for a in range(0, ESD_test_out[1]):\n",
    "            temp_o2_offset = temp_o2_offset.where(temp_o2_offset != ESD_test_out[2][a])\n",
    "\n",
    "    # if there are too few points, set all to nans\n",
    "    if temp_o2_offset.count()<20:\n",
    "        temp_o2_offset[:] = np.nan\n",
    "\n",
    "    # replace nan values with values of temp_o2_offset\n",
    "    gdap_offsets_n_temp['DOXY_ADJUSTED_offset_trimmed'][gdap_offsets_n_temp['main_float_wmo']==[n]] = temp_o2_offset\n",
    "\n",
    "    # append each temp_o2_offset to the new DOXY_ADJUSTED_offset_trimmed vector\n",
    "    # DOXY_ADJUSTED_offset_trimmed.append(temp_o2_offset.values)\n",
    "    #print(len(DOXY_ADJUSTED_offset_trimmed))\n",
    "    break\n",
    "\n",
    "# concatenate all vectors within DOXY_ADJUSTED_offset_trimmed (each represents one WMO)\n",
    "# result_vector = np.concatenate(DOXY_ADJUSTED_offset_trimmed)\n",
    "# # convert to Xarray DataArray\n",
    "# result_da = xr.DataArray(result_vector, dims='N_CROSSOVERS', coords={'N_CROSSOVERS': gdap_offsets_n_temp['N_CROSSOVERS']})\n",
    "# # add to glodap_offsets\n",
    "# gdap_offsets_n_temp['DOXY_ADJUSTED_offset_trimmed']=result_da\n",
    "# print(glodap_offsets)\n",
    "\n",
    "# calculate mean DOXY_ADJUSTED_offsets for different day ranges\n",
    "if len(float_age_bins)>0: # 0 = skip and do not apply\n",
    "    print('in age section')\n",
    "    # create new variables that will be the mean offsets for different time ranges\n",
    "    empty_data = np.empty(len(gdap_offsets_n_temp['N_CROSSOVERS']))\n",
    "    empty_data[:] = np.nan\n",
    "    new_data_array = xr.DataArray(empty_data, coords={'N_CROSSOVERS': gdap_offsets_n_temp['N_CROSSOVERS']}, dims=['N_CROSSOVERS'])\n",
    "\n",
    "    gdap_offsets_n_temp['First_Float_Profile_Date'] = new_data_array.copy()  # for storing the first date of first float profile\n",
    "\n",
    "    for fa in range(len(float_age_bins)-1):\n",
    "        gdap_offsets_n_temp['DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa])] = new_data_array.copy()\n",
    "        gdap_offsets_n_temp['DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa]) + '_count'] = new_data_array.copy()\n",
    "\n",
    "    wmo_list = np.unique(gdap_offsets_n_temp.main_float_wmo)\n",
    "    # loop through all floats\n",
    "    for wmo_n in wmo_list:\n",
    "        print(wmo_n)\n",
    "        # load Sprof file, get date of first profile\n",
    "        argo_n = xr.open_dataset(argo_path + str(wmo_n) + '_Sprof.nc')\n",
    "        first_profile_date = argo_n.JULD[0].values\n",
    "\n",
    "        gdap_offsets_n_temp['First_Float_Profile_Date'][gdap_offsets_n_temp.main_float_wmo==wmo_n] = first_profile_date\n",
    "\n",
    "        # calculate offset time relative to first deployment\n",
    "        time_since_first_ns = gdap_offsets_n_temp.main_float_juld[gdap_offsets_n_temp.main_float_wmo==wmo_n].values-first_profile_date\n",
    "        time_since_first_days = time_since_first_ns / np.timedelta64(1, 'D')\n",
    "\n",
    "        # save out o2 offsets for this float\n",
    "        temp_o2_offset = gdap_offsets_n_temp.DOXY_ADJUSTED_offset[gdap_offsets_n_temp.main_float_wmo==wmo_n]\n",
    "\n",
    "\n",
    "        # now loop through float_age_bins to calculate mean offsets for each age range \n",
    "        for fa in range(len(float_age_bins)-1):\n",
    "            print(fa)\n",
    "            # find ages within range\n",
    "            age_index = np.logical_and(time_since_first_days>=float_age_bins[fa], time_since_first_days<float_age_bins[fa+1])\n",
    "\n",
    "            # save the mean o2 offset where age_index is true to the correct age variable for that float\n",
    "            if not np.all(np.isnan(temp_o2_offset.where(age_index).values)):\n",
    "                gdap_offsets_n_temp['DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa])]\\\n",
    "                    [gdap_offsets_n_temp.main_float_wmo==wmo_n] = np.nanmean(temp_o2_offset.where(age_index).values)\n",
    "            gdap_offsets_n_temp['DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa]) + '_count']\\\n",
    "                                [gdap_offsets_n_temp.main_float_wmo==wmo_n] = (temp_o2_offset.where(age_index).count())\n",
    "\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list to put in True / False if the glodap offsets intersect zero at float mid date\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import functions.sokal_rohlf_calculations as SR\n",
    "from scipy import interpolate\n",
    "\n",
    "CI_confidence = 0.95\n",
    "\n",
    "for idx, filename in enumerate(glodap_offsets_filenames):\n",
    "    for j in range(len(pressure_levels) - 1):\n",
    "\n",
    "        try:\n",
    "            single_level_offsets = gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "            # mean_level_offsets = mean_gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        offsets_g = single_level_offsets.groupby(single_level_offsets.main_float_wmo)\n",
    "        glodap_drift_possible_list = []\n",
    "\n",
    "        for n,g in offsets_g:\n",
    "            g_ox = g.where(~np.isnan(g.DOXY_ADJUSTED_offset_trimmed), drop=True)\n",
    "            if g_ox.dims['N_CROSSOVERS']!=0:\n",
    "\n",
    "                g_ox_sorted = g_ox.sortby(\"glodap_datetime\")\n",
    "                \n",
    "                X_series = pd.Series(mdates.date2num(g_ox_sorted.glodap_datetime))\n",
    "                Y_series = pd.Series(g_ox_sorted.DOXY_ADJUSTED_offset_trimmed.values)\n",
    "\n",
    "                if len(X_series)<5: # too short to realistically do a regression\n",
    "                    uncert_min = np.nan\n",
    "                    uncert_max = np.nan\n",
    "                else:\n",
    "                    # Y_series = pd.Series(g_ox_sorted.DOXY_ADJUSTED_offset_trimmed.values)\n",
    "                    b_yx, a, r2, CI_confidence_slope, ttt, y_err = SR.regress_confidence_sokal_rohlf(X_series, Y_series, CI_confidence)\n",
    "\n",
    "                    if not np.all(np.isnan(a)):\n",
    "\n",
    "                        # extrapolate regression and CI if needed to intersect float_mid_date\n",
    "                        # should apply the slope of the \n",
    "                        float_mid_date = mdates.date2num(g.main_float_juld.mean()) # mean float date in number\n",
    "\n",
    "                        if np.max(X_series)<float_mid_date:\n",
    "                            X_extend = X_series.append(pd.Series(float_mid_date))\n",
    "                            y_extend = a+ X_extend*b_yx\n",
    "\n",
    "                            X_series_last_third = X_series.iloc[np.int64(np.round(len(X_series)*1/2)):-1]\n",
    "                            Y_err_last_third = y_err.iloc[np.int64(np.round(len(X_series)*1/2)):-1]\n",
    "\n",
    "                            b_err, a_err, _, _, _, _ = SR.regress_confidence_sokal_rohlf(X_series_last_third, Y_err_last_third, CI_confidence)\n",
    "                            extrap_error = X_extend.iloc[-1]*b_err+a_err\n",
    "                            y_err_extend = y_err.append(pd.Series(extrap_error))\n",
    "\n",
    "                            uncert_min = y_extend.iloc[-1] - y_err_extend.iloc[-1]\n",
    "                            uncert_max = y_extend.iloc[-1] + y_err_extend.iloc[-1]\n",
    "                        elif np.min(X_series)>float_mid_date:\n",
    "                            X_extend = pd.Series(float_mid_date).append(X_series)\n",
    "                            y_extend = a+ X_extend*b_yx\n",
    "                            X_series_first_half = X_series.iloc[0:np.int64(np.round(len(X_series)*1/2))]\n",
    "                            Y_err_first_half = y_err.iloc[1:np.int64(np.round(len(X_series)*1/2))]\n",
    "                            b_err, a_err, _, _, _, _ = SR.regress_confidence_sokal_rohlf(X_series_last_third, Y_err_last_third, CI_confidence)\n",
    "                            extrap_error = X_extend.iloc[0]*b_err+a_err\n",
    "                            y_err_extend = pd.Series(extrap_error).append(y_err)\n",
    "\n",
    "                #             plt.plot(X_extend, y_extend, color='blue', linestyle = '--')\n",
    "                #             plt.fill_between(X_extend, y_extend-y_err_extend, y_extend+y_err_extend, color='blue', alpha=0.25)    \n",
    "                            uncert_min = y_extend.iloc[0] - y_err_extend.iloc[0]\n",
    "                            uncert_max = y_extend.iloc[0] + y_err_extend.iloc[0]\n",
    "                        else:\n",
    "                            y_float = float_mid_date*b_yx+a\n",
    "                            f = interpolate.interp1d(X_series,y_err)\n",
    "                            y_float_err = f(float_mid_date)\n",
    "\n",
    "                            uncert_min = y_float - y_float_err\n",
    "                            uncert_max = y_float + y_float_err\n",
    "                    #         print(uncert_min)\n",
    "                    #         print(uncert_max)\n",
    "\n",
    "                        \n",
    "                    if np.isnan(uncert_min):\n",
    "                        glodap_drift_possible = np.nan\n",
    "                    elif np.logical_and(uncert_min<0, uncert_max>0):\n",
    "                        glodap_drift_possible = True\n",
    "                    else:\n",
    "                        glodap_drift_possible = False\n",
    "\n",
    "                    glodap_drift_possible_list.append(glodap_drift_possible)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can drift explain offset?\n",
    "count_true = 0\n",
    "count_false = 0\n",
    "for idx, TF in enumerate(glodap_drift_possible_list): \n",
    "    if TF is True: \n",
    "        count_true= count_true+1\n",
    "    elif TF is False:\n",
    "        count_false = count_false+1\n",
    "\n",
    "print('True ' + str(count_true) + ' or ' + str((count_true/len(glodap_drift_possible_list)*100)))\n",
    "print('False ' + str(count_false) + ' or ' + str((count_false/len(glodap_drift_possible_list)*100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are oxygen offsets significantly different from zero?\n",
    "significant=0\n",
    "not_significant = 0\n",
    "no_offset = 0\n",
    "for n,g in offsets_g:\n",
    "    #print(\"n:\", n, \"g:\", g)\n",
    "    g_plot = g.DOXY_ADJUSTED_offset_trimmed\n",
    "\n",
    "    if np.all(np.isnan(g_plot)):\n",
    "        no_offset = no_offset+1\n",
    "        continue \n",
    "    g_mean = np.nanmean(g_plot.values)\n",
    "    t_stat, p_value = stats.ttest_1samp(a=g_plot, popmean=0, nan_policy='omit') ############\n",
    "    \n",
    "    if  p_value<.05:\n",
    "        significant = significant+1\n",
    "    else:\n",
    "        not_significant= not_significant + 1\n",
    "\n",
    "print(\"number significant\", significant)\n",
    "print(\"number not significant\", not_significant)\n",
    "\n",
    "print('number where no offset could be calculated: ' + str(no_offset))\n",
    "print('Total offsets: ' + str(len(offsets_g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, filename in enumerate(glodap_offsets_filenames):\n",
    "    for j in range(len(pressure_levels) - 1):\n",
    "\n",
    "        try:\n",
    "            single_level_offsets = gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "            # mean_level_offsets = mean_gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        offsets_g = single_level_offsets.groupby(single_level_offsets.main_float_wmo)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_level_offsets['p_compare_min'][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth dependency of offsets:\n",
    "# 1. Bin by depth - 50m bins? 100m bins?\n",
    "# 2. Save a dataframe with wmo, depth, o2 conc, o2 offset, o2 offset minus mean o2 offset\n",
    "# That should give you what you need to plot all float offsets vs. depth and concentration\n",
    "press_bin_width = 10\n",
    "press_bins = np.linspace(np.int32(single_level_offsets['p_compare_min'][0].item()), \n",
    "                   np.int32(single_level_offsets['p_compare_max'][0].item()), press_bin_width)\n",
    "\n",
    "press_response_all_df = pd.DataFrame(columns=['wmo','avg_depth','o2_conc', 'o2_offset', 'o2_offset_minus_mean'])\n",
    "num_count = 0\n",
    "for n,g in offsets_g:\n",
    "    num_count = num_count+1    \n",
    "    print(str(n)+ ' ' + str(num_count))\n",
    "    g_ox = g.where(~np.isnan(g.DOXY_ADJUSTED_offset_trimmed), drop=True)\n",
    "\n",
    "    # loop through press_bins, finding pressures that fit and averaging everything\n",
    "    press_response_data = []  # List to store data\n",
    "\n",
    "    for idx in range(0, len(press_bins)-1):\n",
    "        depth_index = np.logical_and(g_ox['PRES_ADJUSTED_float']>=press_bins[idx], \n",
    "                                     g_ox['PRES_ADJUSTED_float']<press_bins[idx+1])\n",
    "        if sum(depth_index)==0:\n",
    "            continue\n",
    "\n",
    "        # make a new dataframe for this pressure bin\n",
    "\n",
    "        press_response_data.append({\n",
    "                    'wmo': np.int32(g_ox['main_float_wmo'][0].item()),\n",
    "                    'avg_depth': np.mean(g_ox['PRES_ADJUSTED_float'][depth_index]).values.tolist(),\n",
    "                    'o2_conc': np.mean(g_ox['DOXY_ADJUSTED_float'][depth_index]).values.tolist(),\n",
    "                    'o2_offset': np.mean(g_ox['DOXY_ADJUSTED_offset_trimmed'][depth_index]).values.tolist(),\n",
    "                    'o2_offset_minus_mean': (np.mean(g_ox['DOXY_ADJUSTED_offset_trimmed'][depth_index]).values - \\\n",
    "                                             np.mean(g_ox['DOXY_ADJUSTED_offset_trimmed']).values).tolist()\n",
    "\n",
    "            })\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    press_response_df = pd.DataFrame(press_response_data)\n",
    "    # concatenate dataframes\n",
    "    press_response_all_df = pd.concat([press_response_all_df, press_response_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "press_response_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# +\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(press_response_all_df['o2_offset_minus_mean'], press_response_all_df['avg_depth'], 'x')\n",
    "plt.xlim([-20, 20])\n",
    "plt.grid()\n",
    "plt.ylim([2050, 1450])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(press_response_all_df['o2_offset_minus_mean'], press_response_all_df['o2_conc'], 'x')\n",
    "plt.xlim([-5, 5])\n",
    "plt.grid()\n",
    "# plt.ylim([2050, 1450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist.statistic.shape)\n",
    "print(offset_bins[0:-1].shape)\n",
    "print(o2_bins[0:-1].shape)\n",
    "print(offset_mesh.T.shape)\n",
    "print(o2_mesh.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o2_bins = np.arange(0, 350, 5)\n",
    "offset_bins = np.arange(-10, 10, 0.5)\n",
    "hist = stats.binned_statistic_2d(press_response_all_df['o2_offset_minus_mean'], press_response_all_df['o2_conc'], None, bins=[offset_bins, o2_bins], statistic=\"count\")\n",
    "\n",
    "hist.statistic[hist.statistic == 0] = np.nan  # Sets all values of 0 to nan as log10(0) = -inf\n",
    "hist.statistic\n",
    "\n",
    "\n",
    "offset_mesh, o2_mesh = np.meshgrid(offset_bins, o2_bins)\n",
    "\n",
    "\n",
    "\n",
    "image = plt.pcolormesh(offset_mesh.T, o2_mesh.T, hist.statistic, shading=\"flat\")\n",
    "\n",
    "# Add colorbar and labels if needed\n",
    "plt.colorbar(label=\"Count\")\n",
    "plt.xlabel(\"Offset\")\n",
    "plt.ylabel(\"Oxygen Concentration\")\n",
    "plt.grid()\n",
    "plt.clim(0,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.all(np.isnan(g_plot)):\n",
    "    no_offset = no_offset+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_level_offsets = gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "offsets_g = single_level_offsets.groupby(single_level_offsets.main_float_wmo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "pressure_level_min = pressure_levels[j]\n",
    "nc_filename_mean = filename[0:-3]+ '_floatmean_withcalibration_depth_grouped_' + \\\n",
    "                                'year_filt_' + str(year_filt) +'_' + str(year_plus_minus) + '_level_' + str(pressure_level_min) + '.nc'\n",
    "\n",
    "nc_filename_mean\n",
    "glodap_offsets_mean = xr.load_dataset(grouped_plot_dir + nc_filename_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_var = glodap_offsets_mean['DOXY_ADJUSTED_offset_trimmed']/glodap_offsets_mean['DOXY_ADJUSTED_glodap']*100\n",
    "temp_var.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms \n",
    "year_filt = 0\n",
    "x_val = 0 # offset in umol / kg\n",
    "# x_val = 1 # offset in % \n",
    "\n",
    "for idx, filename in enumerate(glodap_offsets_filenames):\n",
    "    for j in range(len(pressure_levels) - 1):\n",
    "\n",
    "        pressure_level_min = pressure_levels[j]\n",
    "        nc_filename_mean = filename[0:-3]+ '_floatmean_withcalibration_depth_grouped_' + \\\n",
    "                                'year_filt_' + str(year_filt) +'_' + str(year_plus_minus) + '_level_' + str(pressure_level_min) + '.nc'\n",
    "\n",
    "        # nc_filename_all = filename[0:-3]+ '_all_offsets_depth_grouped_' + \\\n",
    "        #                         'year_filt_' + str(year_filt) +'_' + str(year_plus_minus) + '_level_' + str(pressure_level_min) + '.nc'\n",
    "\n",
    "        # if ~os.path.exists(grouped_plot_dir + nc_filename_mean):\n",
    "        #     continue\n",
    "        # load files and fix longitude\n",
    "        # histogram plot for each file / pressure level\n",
    "        if 'glodap_offsets_mean' in locals():\n",
    "                del glodap_offsets_mean\n",
    "        try:\n",
    "                glodap_offsets_mean = xr.load_dataset(grouped_plot_dir + nc_filename_mean)\n",
    "        except:\n",
    "                continue\n",
    "        \n",
    "                # don't need to fix longitude in this section of code\n",
    "        # float_wmo_list = glodap_offsets_p.index.values\n",
    "        # float_wmo_list = glodap_offsets_mean.main_float_wmo.values\n",
    "        # if 'glodap_offsets' in locals():\n",
    "        #     del glodap_offsets\n",
    "\n",
    "\n",
    "        # glodap_offsets = xr.load_dataset(grouped_plot_dir + nc_filename_all)\n",
    "\n",
    "        # # loop through mean array\n",
    "        # for n in range(0,float_wmo_list.shape[0]):\n",
    "        #         wmo_n = float_wmo_list[n]\n",
    "\n",
    "        #         # fix long_n mean biases:\n",
    "        #         temp_LONGITUDE = glodap_offsets.main_float_longitude.where(glodap_offsets.main_float_wmo==wmo_n, drop=True)\n",
    "\n",
    "        #         if (np.max(temp_LONGITUDE) - np.min(temp_LONGITUDE))>300:\n",
    "        #                 # print(n)\n",
    "        #                 new_mean = np.mean(xr.where(temp_LONGITUDE>=0, temp_LONGITUDE, temp_LONGITUDE+360))\n",
    "        #                 if new_mean>180:\n",
    "        #                         new_mean = new_mean-360\n",
    "                                \n",
    "        #                 old_mean = np.mean(glodap_offsets.main_float_longitude.where(glodap_offsets.main_float_wmo==wmo_n, drop=True))\n",
    "        #                 # print('old mean: '  + str(old_mean.values))\n",
    "        #                 # print('orig mean glodap:' + str(glodap_offsets_mean['main_float_longitude'].loc[{'main_float_wmo': wmo_n}].values))\n",
    "\n",
    "        #                 # print('new mean: '  + str(new_mean.values))\n",
    "\n",
    "        #                 # put new LONGITUDE into long_n\n",
    "        #                 glodap_offsets_mean['main_float_longitude'].loc[{'main_float_wmo': wmo_n}] = new_mean.values\n",
    "        #                 # print('New mean glodap:' + str(glodap_offsets_mean['main_float_longitude'].loc[{'main_float_wmo': wmo_n}].values))\n",
    "\n",
    "        glodap_offsets_p = glodap_offsets_mean.to_dataframe()\n",
    "\n",
    "\n",
    "        parameter_a = 'o2_calib_air_group'\n",
    "        parameter_b = 'pH_group'\n",
    "        offsets_g = glodap_offsets_p.groupby(parameter_a)\n",
    "        offsets_pH = glodap_offsets_p.groupby([parameter_a, parameter_b])\n",
    "        CI_level = 0.95\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "        plt.figure(figsize=(12,12))\n",
    "\n",
    "        #All Float Crossovers\n",
    "        plt.subplot(2,2,1)\n",
    "        if x_val==0:\n",
    "                temp_var = glodap_offsets_mean['DOXY_ADJUSTED_offset_trimmed']\n",
    "                x_label = r'DOXY Offset ($\\mu$mol kg$^{-1}$)'\n",
    "        elif x_val==1:\n",
    "                temp_var = glodap_offsets_mean['DOXY_ADJUSTED_offset_trimmed']/glodap_offsets_mean['DOXY_ADJUSTED_glodap']*100\n",
    "                x_label = 'DOXY Offset (%)'\n",
    "\n",
    "        ncount = temp_var.count()\n",
    "        nmean = np.around(temp_var.mean(), decimals=1)\n",
    "        nstd = np.around(temp_var.std(), decimals=1)\n",
    "        nmedian = np.around(temp_var.median(), decimals=1)\n",
    "        nmin = np.around(temp_var.min(), decimals=1)\n",
    "        nmax = np.around(temp_var.max(), decimals=1)\n",
    "\n",
    "        # temp_var = glodap_offsets_mean['DOXY_ADJUSTED_offset_trimmed']\n",
    "        t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "        CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                        loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                        scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "        CI_low = np.around(CI_vals[0], decimals=1)\n",
    "        CI_high = np.around(CI_vals[1], decimals=1)\n",
    "\n",
    "        o2_offset_data_table = [('All', ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmin, nmax)]\n",
    "        plt.hist(temp_var, \n",
    "                bins=np.linspace(-65, 65, 61), label='n: '+str(ncount.values) +  '\\nmean: ' + str(nmean.values) + '$\\pm$' + str(nstd.values) + '\\nmed: ' + str(nmedian.values) +\n",
    "                '\\nCI: {' + str(CI_low) +',' +  str(CI_high) + '}'\n",
    "                ) # ,label=str(n)\n",
    "        #print(np.around(glodap_offsets_mean['DOXY_ADJUSTED_offset_trimmed'].median().values, decimals=1))\n",
    "        plt.grid()\n",
    "        plt.title('All float crossovers')\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel('Count')\n",
    "        plt.axvline(x=0, color='k')\n",
    "        plt.legend()\n",
    "\n",
    "        # All float crossovers, zoomed in\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.title('All floats, $\\pm$ 20 $\\mu$mol kg$^{-1}$ range')\n",
    "\n",
    "        plt.hist(temp_var, \n",
    "                bins=np.linspace(-20, 20, 50),label='median='+str(nmedian.values) + \n",
    "                ', n='+str(ncount.values)) # ,label=str(n)\n",
    "        plt.grid()\n",
    "        plt.ylabel('Count')\n",
    "        plt.axvline(x=0, color='k')\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "\n",
    "        # Split into calibration group\n",
    "        plt.subplot(2,2,3)\n",
    "        No_air_cal_all = offsets_g.get_group('no air cal')\n",
    "        if x_val==0:\n",
    "                temp_var = No_air_cal_all['DOXY_ADJUSTED_offset_trimmed']\n",
    "        elif x_val==1:\n",
    "                temp_var = No_air_cal_all['DOXY_ADJUSTED_offset_trimmed']/No_air_cal_all['DOXY_ADJUSTED_glodap']*100\n",
    "\n",
    "        nmean = np.around(temp_var.mean(), decimals=1)\n",
    "        nstd = np.around(temp_var.std(), decimals=1)\n",
    "        nmedian = np.around(temp_var.median(), decimals=1)\n",
    "        ncount = temp_var.count() # only counts non-nan values\n",
    "        nmin = np.around(temp_var.min(), decimals=1)\n",
    "        nmax = np.around(temp_var.max(), decimals=1)\n",
    "\n",
    "        # temp_var = No_air_cal_all['DOXY_ADJUSTED_offset_trimmed']\n",
    "        t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "        CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                        loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                        scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "        CI_low = np.around(CI_vals[0], decimals=1)\n",
    "        CI_high = np.around(CI_vals[1], decimals=1)\n",
    "        o2_offset_data_table.append(('No air cal.', ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmin, nmax))\n",
    "\n",
    "\n",
    "        plt.hist(temp_var, bins=np.linspace(-20, 20, 50),\n",
    "                    alpha=0.5,label='No air cal.: \\nn: '+str(ncount) +  '\\nmean: ' + str(nmean) + '$\\pm$' + str(nstd) + '\\nmed: ' + str(nmedian) +\n",
    "                '\\nCI: {' + str(CI_low) +',' +  str(CI_high) + '}\\n')\n",
    "\n",
    "\n",
    "\n",
    "        air_cal_all = offsets_g.get_group('air cal')\n",
    "        if x_val==0:\n",
    "                temp_var = air_cal_all['DOXY_ADJUSTED_offset_trimmed']\n",
    "        elif x_val==1:\n",
    "                temp_var = air_cal_all['DOXY_ADJUSTED_offset_trimmed']/air_cal_all['DOXY_ADJUSTED_glodap']*100\n",
    "\n",
    "        nmean = np.around(temp_var.mean(), decimals=1)\n",
    "        nstd = np.around(temp_var.std(), decimals=1)\n",
    "        nmedian = np.around(temp_var.median(), decimals=1)\n",
    "        ncount = temp_var.count() # only counts non-nan values\n",
    "        nmin = np.around(temp_var.min(), decimals=1)\n",
    "        nmax = np.around(temp_var.max(), decimals=1)\n",
    "\n",
    "        # temp_var = air_cal_all['DOXY_ADJUSTED_offset_trimmed']\n",
    "        t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "        CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                        loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                        scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "        CI_low = np.around(CI_vals[0], decimals=1)\n",
    "        CI_high = np.around(CI_vals[1], decimals=1)\n",
    "\n",
    "        o2_offset_data_table.append(('Air cal.', ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmin, nmax))\n",
    "\n",
    "\n",
    "        plt.hist(temp_var, bins=np.linspace(-20, 20, 50),\n",
    "                    alpha=0.5,label='Air cal.: \\nn: '+str(ncount) +  '\\nmean: ' + str(nmean) + '$\\pm$' + str(nstd) + '\\nmed: ' + str(nmedian) +\n",
    "                '\\nCI: {' + str(CI_low) +',' +  str(CI_high) + '}\\n')\n",
    "\n",
    "\n",
    "        no_cal_all = offsets_g.get_group('no cal/bad')\n",
    "        if x_val==0:\n",
    "                temp_var = no_cal_all['DOXY_ADJUSTED_offset_trimmed']\n",
    "        elif x_val==1:\n",
    "                temp_var = no_cal_all['DOXY_ADJUSTED_offset_trimmed']/no_cal_all['DOXY_ADJUSTED_glodap']*100\n",
    "\n",
    "        nmean = np.around(temp_var.mean(), decimals=1)\n",
    "        nstd = np.around(temp_var.std(), decimals=1)\n",
    "        nmedian = np.around(temp_var.median(), decimals=1)\n",
    "        ncount = temp_var.count() # only counts non-nan values\n",
    "        nmin = np.around(temp_var.min(), decimals=1)\n",
    "        nmax = np.around(temp_var.max(), decimals=1)\n",
    "\n",
    "        # temp_var = no_cal_all['DOXY_ADJUSTED_offset_trimmed']\n",
    "        t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "        CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                        loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                        scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "        CI_low = np.around(CI_vals[0], decimals=1)\n",
    "        CI_high = np.around(CI_vals[1], decimals=1)\n",
    "        o2_offset_data_table.append(('No cal.', ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmin, nmax))\n",
    "\n",
    "        plt.hist(temp_var, bins=np.linspace(-20, 20, 50),\n",
    "                    alpha=0.5,label='Bad or no cal.: \\nn: '+str(ncount) +  '\\nmean: ' + str(nmean) + '$\\pm$' + str(nstd) + '\\nmed: ' + str(nmedian) +\n",
    "                '\\nCI: {' + str(CI_low) +',' +  str(CI_high) + '}')\n",
    "\n",
    "\n",
    "\n",
    "        #for n, group in offsets_g:\n",
    "        #    print(n)\n",
    "        #    nmean = np.around(group['DOXY_ADJUSTED_offset_trimmed'].mean(), decimals=1)\n",
    "        #    nmedian = np.around(group['DOXY_ADJUSTED_offset_trimmed'].median(), decimals=1)\n",
    "        #    ncount = group['DOXY_ADJUSTED_offset_trimmed'].count()\n",
    "\n",
    "            \n",
    "\n",
    "        plt.title('Grouped by calibration approach')\n",
    "\n",
    "        plt.axvline(x=0, color='k')\n",
    "        plt.grid()\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel('Count')\n",
    "\n",
    "        plt.legend()\n",
    "\n",
    "        # pH equipped floats only \n",
    "        plt.subplot(2,2,4)\n",
    "        plt.title('pH equipped floats only')\n",
    "\n",
    "        no_air_cal_ph = offsets_pH.get_group(('no air cal', 'pH'))\n",
    "        if x_val==0:\n",
    "                temp_var = no_air_cal_ph['DOXY_ADJUSTED_offset_trimmed']\n",
    "        elif x_val==1:\n",
    "                temp_var = no_air_cal_ph['DOXY_ADJUSTED_offset_trimmed']/no_air_cal_ph['DOXY_ADJUSTED_glodap']*100\n",
    "\n",
    "        nmean = np.around(temp_var.mean(), decimals=1)\n",
    "        nmedian = np.around(temp_var.median(), decimals=1)\n",
    "        nstd = np.around(temp_var.std(), decimals=1)\n",
    "        nmin = np.around(temp_var.min(), decimals=1)\n",
    "        nmax = np.around(temp_var.max(), decimals=1)\n",
    "\n",
    "        ncount = temp_var.count()\n",
    "\n",
    "        # temp_var = no_air_cal_ph['DOXY_ADJUSTED_offset_trimmed']\n",
    "        t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "        CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                        loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                        scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "        CI_low = np.around(CI_vals[0], decimals=1)\n",
    "        CI_high = np.around(CI_vals[1], decimals=1)\n",
    "        o2_offset_data_table.append(('pH - No cal.', ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmin, nmax))\n",
    "\n",
    "        plt.hist(temp_var, bins=np.linspace(-20, 20, 50),\n",
    "                    alpha=0.5,label='No air cal. \\nn: '+str(ncount) +  '\\nmean: ' + str(nmean) + '$\\pm$' + str(nstd) + '\\nmed: ' + str(nmedian) +\n",
    "                '\\nCI: {' + str(CI_low) +',' +  str(CI_high) + '}\\n')\n",
    "\n",
    "        air_cal_ph = offsets_pH.get_group(('air cal', 'pH'))\n",
    "        if x_val==0:\n",
    "                temp_var = air_cal_ph['DOXY_ADJUSTED_offset_trimmed']\n",
    "        elif x_val==1:\n",
    "                temp_var = air_cal_ph['DOXY_ADJUSTED_offset_trimmed']/air_cal_ph['DOXY_ADJUSTED_glodap']*100\n",
    "\n",
    "        nmean = np.around(temp_var.mean(), decimals=1)\n",
    "        nmedian = np.around(temp_var.median(), decimals=1)\n",
    "        ncount = temp_var.count()\n",
    "        nstd = np.around(temp_var.std(), decimals=1)\n",
    "        nmin = np.around(temp_var.min(), decimals=1)\n",
    "        nmax = np.around(temp_var.max(), decimals=1)\n",
    "\n",
    "        # temp_var = air_cal_ph['DOXY_ADJUSTED_offset_trimmed']\n",
    "        t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "        CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                        loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                        scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "        CI_low = np.around(CI_vals[0], decimals=1)\n",
    "        CI_high = np.around(CI_vals[1], decimals=1)\n",
    "        o2_offset_data_table.append(('pH - Air cal.', ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmin, nmax))\n",
    "\n",
    "        plt.hist(temp_var, bins=np.linspace(-20, 20, 50),\n",
    "                    alpha=0.4,label='Air cal. \\nn: '+str(ncount) +  '\\nmean: ' + str(nmean) + '$\\pm$' + str(nstd) + '\\nmed: ' + str(nmedian) +\n",
    "                '\\nCI: {' + str(CI_low) +',' +  str(CI_high) + '}')\n",
    "\n",
    "\n",
    "        #for n, group in offsets_pH:\n",
    "        #    if n[1] == 'no pH' or n[0] == 'no cal/bad':\n",
    "        #        continue\n",
    "        #    print(n)\n",
    "        #    nmean = np.around(group['DOXY_ADJUSTED_offset_trimmed'].mean(), decimals=1)\n",
    "        #    nmedian = np.around(group['DOXY_ADJUSTED_offset_trimmed'].median(), decimals=1)\n",
    "        #    ncount = group['DOXY_ADJUSTED_offset_trimmed'].count()\n",
    "\n",
    "        plt.xlabel(x_label)\n",
    "        plt.ylabel('Count')\n",
    "        plt.axvline(x=0, color='k')\n",
    "\n",
    "        plt.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.grid()\n",
    "\n",
    "        if x_val==0:\n",
    "                plot_filename = nc_filename_mean\n",
    "        elif x_val==1:\n",
    "                plot_filename = nc_filename_mean + '_offset_percent'\n",
    "\n",
    "        plt.savefig(grouped_plot_dir + plot_filename + '_histogram.png', dpi=300)\n",
    "\n",
    "        print(tabulate(o2_offset_data_table, headers=['count', 'mean', 'std', 'p_value', str(CI_level*100) + '% CI low', str(CI_level*100) + '% CI high', 'median', 'min', 'max'], tablefmt='grid'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot nitrate, pH, DIC offsets for each file / grouping \n",
    "CI_level = 0.95\n",
    "var_plot = ['NITRATE_ADJUSTED','pH_25C_TOTAL_ADJUSTED', 'DIC']\n",
    "\n",
    "# Plotting histograms \n",
    "year_filt = 0\n",
    "x_val = 0 # offset in umol / kg\n",
    "\n",
    "for idx, filename in enumerate(glodap_offsets_filenames):\n",
    "    for j in range(len(pressure_levels) - 1):\n",
    "\n",
    "                pressure_level_min = pressure_levels[j]\n",
    "                nc_filename_mean = filename[0:-3]+ '_floatmean_withcalibration_depth_grouped_' + \\\n",
    "                                'year_filt_' + str(year_filt) +'_' + str(year_plus_minus) + '_level_' + str(pressure_level_min) + '.nc'\n",
    "\n",
    "                if 'glodap_offsets_mean' in locals():\n",
    "                    del glodap_offsets_mean\n",
    "                try:\n",
    "                        glodap_offsets_mean = xr.load_dataset(grouped_plot_dir + nc_filename_mean)\n",
    "                except:\n",
    "                        continue\n",
    "                \n",
    "                # plt.rcParams.update({'font.size': 12})\n",
    "                o2_offset_data_table = []\n",
    "               \n",
    "                plt.figure(figsize=(12,5))\n",
    "\n",
    "                for v, var in enumerate(var_plot):\n",
    "                    if np.logical_or(var=='PH_IN_SITU_TOTAL_ADJUSTED', var=='pH_25C_TOTAL_ADJUSTED'):\n",
    "                        scale = 1000\n",
    "                        x_label_units = '(mpH)'\n",
    "                    else:\n",
    "                        scale = 1\n",
    "                        x_label_units = '($\\mu$mol kg$^{-1}$)'\n",
    "                    #All Float Crossovers\n",
    "                    plt.subplot(1,3,v+1)\n",
    "                    ncount = glodap_offsets_mean[var_plot[v] + '_offset'].count().values\n",
    "                    nmean = np.around(glodap_offsets_mean[var_plot[v] + '_offset'].mean().values*scale, decimals=1)\n",
    "                    nstd = np.around(glodap_offsets_mean[var_plot[v] + '_offset'].std().values*scale, decimals=1)\n",
    "                    nmedian = np.around(glodap_offsets_mean[var_plot[v] + '_offset'].median().values*scale, decimals=1)\n",
    "                    nmin = np.around(glodap_offsets_mean[var_plot[v] + '_offset'].min().values*scale, decimals=1)\n",
    "                    nmax = np.around(glodap_offsets_mean[var_plot[v] + '_offset'].max().values*scale, decimals=1)\n",
    "\n",
    "                    temp_var = glodap_offsets_mean[var_plot[v] + '_offset']*scale\n",
    "                    t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "                    CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                                    loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                                    scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "                    CI_low = np.around(CI_vals[0], decimals=2)\n",
    "                    CI_high = np.around(CI_vals[1], decimals=2)\n",
    "                    o2_offset_data_table.append([var_plot[v], \n",
    "                                                ncount, \n",
    "                                                nmean, \n",
    "                                                nstd, \n",
    "                                                p_value, CI_low, CI_high, nmedian, nmin, nmax])\n",
    "                    plt.hist(glodap_offsets_mean[var_plot[v] + '_offset']*scale, \n",
    "                            label='n: '+str(ncount) +  '\\nmean: ' + str(nmean) + '$\\pm$' + str(nstd) + \n",
    "                        '\\nCI: {' + str(CI_low) +',' +  str(CI_high) + '}',bins=25) # ,label=str(n) #bins=np.linspace(-65, 65, 61),\n",
    "                    #print(np.around(glodap_offsets_mean['DOXY_ADJUSTED_offset_trimmed'].median().values, decimals=1))\n",
    "                    plt.grid()\n",
    "                    plt.title('All float crossovers')\n",
    "                    plt.xlabel(var_plot[v] + ' Offset ' + x_label_units)\n",
    "                    plt.ylabel('Count')\n",
    "                    plt.axvline(x=0, color='k')\n",
    "                    plt.legend(fontsize=9)\n",
    "                plt.tight_layout\n",
    "\n",
    "                plot_filename = nc_filename_mean\n",
    "\n",
    "                plt.savefig(grouped_plot_dir + plot_filename + '_no3_pH_dic_histogram.png', dpi=300)\n",
    "\n",
    "                print(tabulate(o2_offset_data_table, headers=['count', 'mean', 'std', 'p_value', str(CI_level*100) + '% CI low', str(CI_level*100) + '% CI high', 'median', 'min', 'max'], tablefmt='grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_gdap_offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting group names for projects, sensor types, data centers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'project_name' is the variable in your xarray dataset\n",
    "dataset_n = mean_gdap_offsets[filename]['level_1500']\n",
    "project_name_values = dataset_n['project_name'].values\n",
    "big_projects = []\n",
    "# Get unique values and their counts\n",
    "unique_groups, group_counts = np.unique(project_name_values, return_counts=True)\n",
    "\n",
    "# Display the group names and counts\n",
    "for group_name, count in zip(unique_groups, group_counts):\n",
    "    print(f\"Group Name: {group_name}, Count: {count}\")\n",
    "    \n",
    "    if count>=10:\n",
    "        big_projects.append(group_name)\n",
    "\n",
    "print(big_projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n = mean_gdap_offsets[filename]['level_1500']\n",
    "data_center_values = dataset_n['data_centre'].values\n",
    "data_centers = []\n",
    "# Get unique values and their counts\n",
    "unique_groups, group_counts = np.unique(data_center_values, return_counts=True)\n",
    "\n",
    "# Display the group names and counts\n",
    "for group_name, count in zip(unique_groups, group_counts):\n",
    "    print(f\"Group Name: {group_name}, Count: {count}\")\n",
    "    \n",
    "    if count>=5:\n",
    "        data_centers.append(group_name)\n",
    "\n",
    "print(data_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n = mean_gdap_offsets[filename]['level_1500']\n",
    "o2_sensor_values = dataset_n['DOXY_sensor'].values\n",
    "o2_sensors = []\n",
    "# Get unique values and their counts\n",
    "unique_groups, group_counts = np.unique(o2_sensor_values, return_counts=True)\n",
    "\n",
    "# Display the group names and counts\n",
    "for group_name, count in zip(unique_groups, group_counts):\n",
    "    print(f\"Group Name: {group_name}, Count: {count}\")\n",
    "    \n",
    "    if count>=1:\n",
    "        o2_sensors.append(group_name)\n",
    "\n",
    "print(o2_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([float_age_bins[fa], float_age_bins[fa+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n.where(abs(dataset_n['main_float_latitude'])<60)\n",
    "dataset_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_mag_p = temp_dataset['DOXY_ADJUSTED_float'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # want to loop through and save out all pressure values for each type (cal_group) so that I can make some synthesis plots\n",
    "\n",
    "# cal_groups_all  = [['All', 'air cal', 'no air cal'], o2_sensors, big_projects, data_centers]\n",
    "# parameter_a_all= ['o2_calib_air_group', 'DOXY_sensor', 'project_name', 'data_centre']\n",
    "\n",
    "# cal_groups = big_projects\n",
    "# parameter_a = 'project_name'\n",
    "\n",
    "# cal_groups = data_centers\n",
    "# parameter_a = 'data_centre'\n",
    "\n",
    "# cal_groups = ['All', 'air cal', 'no air cal']\n",
    "# parameter_a = 'o2_calib_air_group'\n",
    "\n",
    "# cal_groups = big_projects\n",
    "# parameter_a = 'project_name'\n",
    "\n",
    "cal_groups  = o2_sensors\n",
    "parameter_a= 'DOXY_sensor' \n",
    "\n",
    "# cal_groups = data_centers\n",
    "# parameter_a = 'data_centre'\n",
    "\n",
    "CI_level = 0.95\n",
    "\n",
    "float_age_filter = 1\n",
    "# for n, cal_groups in enumerate(cal_groups_all):\n",
    "#     parameter_a = parameter_a_all[n]\n",
    "\n",
    "abs_lat_cutoff_shallow = 55\n",
    "\n",
    "for fa in range(len(float_age_bins)-1):\n",
    "    o2_offset_data_table = []\n",
    "\n",
    "    fig = plt.figure(figsize=(34,len(cal_groups)*4))\n",
    "\n",
    "    for filename in glodap_offsets_filenames:\n",
    "        \n",
    "        for j in range(len(pressure_levels)-1):\n",
    "            try:\n",
    "                dataset_n = mean_gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "            except:\n",
    "                continue   \n",
    "\n",
    "            # if pressure levels are too shallow, remove data in high latitudes given issues with stratification\n",
    "            if pressure_levels[j]<500:\n",
    "                dataset_n = dataset_n.where(abs(dataset_n['main_float_latitude'])<abs_lat_cutoff_shallow)\n",
    "\n",
    "              \n",
    "            for idx, cal in enumerate(cal_groups):\n",
    "                dataset_p = dataset_n.to_dataframe()\n",
    "                offsets_o2_cal_group = dataset_p.groupby(parameter_a)\n",
    "                # offsets_pH = dataset_p.groupby([parameter_a, parameter_b])\n",
    "\n",
    "                if float_age_filter==0:\n",
    "                    var = 'DOXY_ADJUSTED_offset_trimmed'\n",
    "                else:\n",
    "                    var = 'DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa])\n",
    "                    age_title = str(float_age_bins[fa]) + ' to ' + str(float_age_bins[fa+1]) + 'days '\n",
    "\n",
    "                if np.logical_and(idx==0, parameter_a=='o2_calib_air_group'):\n",
    "                    # print(idx)\n",
    "                    # print(cal)\n",
    "                    data_p = dataset_n[var].copy()\n",
    "                    pressure_p = dataset_n.PRES_ADJUSTED_float\n",
    "                    data_mag_p = dataset_n['DOXY_ADJUSTED_float'].copy()\n",
    "                    if float_age_filter==1:\n",
    "                        # remove data with too few points if looking at float age - otherwise it already happened during the pressure level filtering\n",
    "                        count_p = dataset_n[var + '_count']\n",
    "                        data_p[count_p < 20] = np.nan\n",
    "\n",
    "                else:\n",
    "                    temp_dataset = offsets_o2_cal_group.get_group(cal)\n",
    "                    data_p = temp_dataset[var].copy()\n",
    "                    pressure_p = temp_dataset.PRES_ADJUSTED_float\n",
    "                    data_mag_p = temp_dataset['DOXY_ADJUSTED_float'].copy()\n",
    "\n",
    "                    if float_age_filter==1:\n",
    "                        # remove data with too few points if looking at float age - otherwise it already happened during the pressure level filtering\n",
    "                        count_p = temp_dataset[var + '_count']\n",
    "                        data_p[count_p < 20] = np.nan\n",
    "\n",
    "            \n",
    "\n",
    "                nmean = np.around(data_p.mean(), decimals=1)\n",
    "                ncount = np.around(data_p.count(), decimals=1)\n",
    "                nstd = np.around(data_p.std(), decimals=1)\n",
    "                nmedian = np.around(data_p.median(), decimals=1)\n",
    "\n",
    "                percent_sat_mag = data_p/data_mag_p*100\n",
    "                nmedian_percent_mag = np.around(percent_sat_mag.median(), decimals=1)\n",
    "\n",
    "                temp_var = data_p.copy()\n",
    "                t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "                CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                                loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                                scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "                CI_low = np.around(CI_vals[0], decimals=1)\n",
    "                CI_high = np.around(CI_vals[1], decimals=1)\n",
    "                ax1 = plt.subplot(len(cal_groups),4,(1+4*idx))\n",
    "                if float_age_filter==0:\n",
    "                    plt.title(cal.strip() )\n",
    "                else:\n",
    "                    plt.title(cal.strip() + ' ' + age_title)\n",
    "                plt.plot(data_p, pressure_p,'.')\n",
    "                plt.errorbar(nmean, pressure_p.mean(), xerr=nmean-CI_low, fmt='o', capsize=5,color='k')\n",
    "\n",
    "                plt.grid('on')\n",
    "                ax2 = plt.subplot(len(cal_groups),4,(2+4*idx))\n",
    "                plt.title('+/- 20 umol/kg')\n",
    "                plt.plot(data_p, pressure_p,'.')\n",
    "                plt.errorbar(nmean, pressure_p.mean(), xerr=nmean-CI_low, fmt='o', capsize=5,color='k')\n",
    "\n",
    "                npres = np.around(pressure_p.mean(), decimals=1)\n",
    "                plt.xlim([-20, 20])\n",
    "                plt.grid('on')\n",
    "                o2_offset_data_table.append((idx, npres, ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmedian_percent_mag))\n",
    "\n",
    "                # plot offset in percent saturation\n",
    "                ax2 = plt.subplot(len(cal_groups),4,(3+4*idx))\n",
    "                plt.title('Median percent Sat')\n",
    "                plt.plot(percent_sat_mag, pressure_p,'.')\n",
    "                plt.plot(nmedian_percent_mag, pressure_p.mean(),'sk', markersize=10, linewidth=10)\n",
    "\n",
    "                # plt.errorbar(nmean, pressure_p.mean(), xerr=nmean-CI_low, fmt='o', capsize=5,color='k')\n",
    "\n",
    "                plt.xlim([-10, 10])\n",
    "                plt.grid('on')\n",
    "\n",
    "    for idx, cal in enumerate(cal_groups):\n",
    "        plt.subplot(len(cal_groups),4,(1+4*idx))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.plot([0,0], [0, 2000], 'k-')\n",
    "        plt.subplot(len(cal_groups),4,(2+4*idx))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.plot([0,0], [0, 2000], 'k-')\n",
    "\n",
    "        plt.subplot(len(cal_groups),4,(3+4*idx))\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.plot([0,0], [0, 2000], 'k-')\n",
    "\n",
    "    o2_offset_data_table = [row for row in o2_offset_data_table if not np.isnan(row[1])]\n",
    "    sorted_data_table = sorted(o2_offset_data_table, key=lambda x: x[0])\n",
    "\n",
    "    # base_value = 1\n",
    "    # increment = 3\n",
    "    # num_iterations = 10  \n",
    "\n",
    "    # for idx in range(num_iterations):\n",
    "    #     current_value = base_value + increment * idx\n",
    "    #     print(f\"Index: {idx}, Value: {current_value}\")\n",
    "\n",
    "    for idx, cal in enumerate(cal_groups):\n",
    "        filtered_data_table = [row for row in sorted_data_table if row[0] == idx]\n",
    "        # Create a string buffer to store the table\n",
    "        table_buffer = StringIO()\n",
    "\n",
    "        # Convert the o2_offset_data_table to a table and write it to the buffer\n",
    "        print(tabulate(filtered_data_table, \n",
    "                    headers=['Cal group', 'Pressure', 'count',\n",
    "                                'mean', 'std', 'p_value', \n",
    "                                str(CI_level*100) + '% CI low', str(CI_level*100) + '% CI high', \n",
    "                                'median', 'med_per'], tablefmt='grid'), file=table_buffer)\n",
    "\n",
    "        # Display the table in a subplot\n",
    "        plt.subplot(len(cal_groups),4,(4+4*idx))\n",
    "        plt.title(cal.strip())\n",
    "\n",
    "        plt.axis('off')  # Turn off axis for the table\n",
    "        # plt.text(0.01, 0.01, table_buffer.getvalue(), va='top', ha='left', fontfamily='monospace', fontsize=8)\n",
    "        plt.text(0.01, 0.95, table_buffer.getvalue(), va='top', ha='left', transform=plt.gca().transAxes, fontfamily='monospace', fontsize=8)\n",
    "        \n",
    "        # break\n",
    "\n",
    "    plot_filename = var + '_vs_depth_grouped_' + parameter_a + '_year_filt_' + str(year_filt) +'_' + str(year_plus_minus)  + '_abs_Lat_cut_' + str(abs_lat_cutoff_shallow)\n",
    "    plt.savefig(grouped_plot_dir + plot_filename + '.png', dpi=300)\n",
    "\n",
    "    o2_offset_table_cal_groups = pd.DataFrame(columns=['Cal group','pressure','offset', 'CI_low', 'CI_high', 'n_count', 'per_med'])\n",
    "    \n",
    "    for idx, cal in enumerate(cal_groups):\n",
    "        filtered_data_table = [row for row in sorted_data_table if row[0] == idx]\n",
    "\n",
    "        for i in range(0,len(filtered_data_table)):\n",
    "            new_data_o2_direct_impact = [(cal.strip(), \n",
    "                                        filtered_data_table[i][1], \n",
    "                                        filtered_data_table[i][3],\n",
    "                                        filtered_data_table[i][6],\n",
    "                                        filtered_data_table[i][7],\n",
    "                                        filtered_data_table[i][2],\n",
    "                                        filtered_data_table[i][9])]\n",
    "            new_data_o2_direct_impact_df = pd.DataFrame(new_data_o2_direct_impact, columns=['Cal group','pressure','offset', 'CI_low', 'CI_high', 'n_count', 'per_med'])\n",
    "            o2_offset_table_cal_groups = pd.concat([o2_offset_table_cal_groups, new_data_o2_direct_impact_df], ignore_index=True)\n",
    "\n",
    "    if parameter_a=='project_name':\n",
    "        n_cutoff= 10\n",
    "        x_lim = [-25, 10]\n",
    "    elif parameter_a=='DOXY_sensor':\n",
    "        n_cutoff = 5\n",
    "        x_lim = [-15, 5]\n",
    "    elif parameter_a == 'o2_calib_air_group':\n",
    "        n_cutoff = 0\n",
    "        x_lim = [-10, 2.5]\n",
    "    elif parameter_a == 'data_centre':\n",
    "        n_cutoff = 5\n",
    "        x_lim = [-22, 5]\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = plt.subplot(1,2,1)\n",
    "    ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "    for cal in cal_groups:\n",
    "        temp_group = o2_offset_table_cal_groups[o2_offset_table_cal_groups['Cal group']==cal.strip()]\n",
    "\n",
    "        if temp_group.n_count.values.mean()<n_cutoff:\n",
    "            continue\n",
    "\n",
    "        line, = ax1.plot(temp_group.offset, temp_group.pressure, label=cal.strip() + ' (' + str(temp_group.n_count.values.mean()) + ')')\n",
    "        line, = ax2.plot(temp_group.per_med, temp_group.pressure, label=cal.strip() + ' (' + str(temp_group.n_count.values.mean()) + ')')\n",
    "\n",
    "        line_color = line.get_color()\n",
    "        for i in range(0,len(temp_group)):\n",
    "            ax1.errorbar(temp_group.offset.values[i], temp_group.pressure.values[i], xerr=temp_group.offset.values[i]-temp_group.CI_low.values[i],\n",
    "                        fmt='o', capsize=5, color=line_color)\n",
    "\n",
    "    ax1.invert_yaxis()\n",
    "    ax2.invert_yaxis()\n",
    "\n",
    "    ax1.plot([0,0], [0, 2000], 'k-')\n",
    "    ax2.plot([0,0], [0, 2000], 'k-')\n",
    "\n",
    "    ax1.grid('on')\n",
    "    ax2.grid('on')\n",
    "\n",
    "    ax1.set_xlim(x_lim)\n",
    "    ax1.legend()\n",
    "    if float_age_filter==0:\n",
    "        ax1.set_title(parameter_a + ' cutoff of ' + str(n_cutoff) + ' floats' + ' year_filt=' + str(year_filt))\n",
    "\n",
    "    else:\n",
    "        ax1.set_title(parameter_a + ' cutoff of ' + str(n_cutoff) + ' floats' + ' year_filt=' + str(year_filt) + ' ' + age_title)\n",
    "    ax2.set_title('median_percent')\n",
    "\n",
    "    ax1.set_xlabel('umol/kg')\n",
    "    ax2.set_xlabel('%')\n",
    "\n",
    "    ax1.set_ylabel('Pressure')\n",
    "    ax2.set_ylabel('Pressure')\n",
    "\n",
    "    plot_filename = var + 'Offsets_vs_depth_grouped_' + parameter_a + '_year_filt' + str(year_filt) +'_' + str(year_plus_minus) + '_all_cutoff_' + str(n_cutoff)  + '_abs_Lat_cut_' + str(abs_lat_cutoff_shallow)\n",
    "    fig.savefig(grouped_plot_dir  + plot_filename + '.png', dpi=600)\n",
    "\n",
    "    \n",
    "    if float_age_filter==0:\n",
    "        break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting multiple groupings of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_2_filter['o2_calib_comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to find the one SBE63 with air cal so I can check it \n",
    "dataset_n = mean_gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "# cal_a = cal_groups_a[0] # air cal \n",
    "cal_a = cal_groups_a[1] # no air cal \n",
    "\n",
    "cal_a\n",
    "dataset_1_filter = dataset_n.where(dataset_n[parameter_a]==cal_a, drop=True)\n",
    "dataset_1_filter\n",
    "# cal_b = cal_groups_b[7] # SBE 63\n",
    "cal_b = cal_groups_b[2] # 4330\n",
    "cal_b = cal_groups_b[1] # 4330\n",
    "\n",
    "print(cal_b)\n",
    "dataset_2_filter = dataset_n.where(dataset_1_filter[parameter_b]==cal_b, drop=True)\n",
    "for i in range(0, len(dataset_2_filter['o2_calib_comment'])):\n",
    "    print(dataset_2_filter['o2_calib_comment'][i].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # multiple groupings of parameters\n",
    "\n",
    "# cal_groups_all  = [['All', 'air cal', 'no air cal'], o2_sensors, big_projects, data_centers]\n",
    "# parameter_a_all= ['o2_calib_air_group', 'DOXY_sensor', 'project_name', 'data_centre']\n",
    "\n",
    "# cal_groups = big_projects\n",
    "# parameter_a = 'project_name'\n",
    "\n",
    "# cal_groups = data_centers\n",
    "# parameter_a = 'data_centre'\n",
    "\n",
    "\n",
    "\n",
    "# cal_groups = big_projects\n",
    "# parameter_a = 'project_name'\n",
    "\n",
    "cal_groups_b  = o2_sensors\n",
    "parameter_b= 'DOXY_sensor' \n",
    "\n",
    "cal_groups_a = ['air cal', 'no air cal']\n",
    "parameter_a = 'o2_calib_air_group'\n",
    "\n",
    "\n",
    "# cal_groups = data_centers\n",
    "# parameter_a = 'data_centre'\n",
    "\n",
    "CI_level = 0.95\n",
    "\n",
    "float_age_filter = 0\n",
    "# for n, cal_groups in enumerate(cal_groups_all):\n",
    "#     parameter_a = parameter_a_all[n]\n",
    "\n",
    "abs_lat_cutoff_shallow = 55\n",
    "\n",
    "\n",
    "# AA = dataset_n.where(dataset_n['o2_calib_air_group']=='air cal', drop=True)\n",
    "# BB = AA.where(AA['DOXY_sensor']==o2_sensors[1], drop=True)\n",
    "\n",
    "for fa in range(len(float_age_bins)-1):\n",
    "    o2_offset_data_table = []\n",
    "\n",
    "    fig = plt.figure(figsize=(20,len(cal_groups_a)*10))\n",
    "\n",
    "    for filename in glodap_offsets_filenames:\n",
    "        \n",
    "        for j in range(len(pressure_levels)-1):\n",
    "            try:\n",
    "                dataset_n = mean_gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "            except:\n",
    "                continue   \n",
    "\n",
    "            # if pressure levels are too shallow, remove data in high latitudes given issues with stratification\n",
    "            if pressure_levels[j]<500:\n",
    "                dataset_n = dataset_n.where(abs(dataset_n['main_float_latitude'])<abs_lat_cutoff_shallow)\n",
    "\n",
    "              \n",
    "            for idx, cal_a in enumerate(cal_groups_a):\n",
    "                dataset_1_filter = dataset_n.where(dataset_n[parameter_a]==cal_a, drop=True)\n",
    "\n",
    "                for idx2, cal_b in enumerate(cal_groups_b):\n",
    "                    dataset_2_filter = dataset_n.where(dataset_1_filter[parameter_b]==cal_b, drop=True)\n",
    "\n",
    "                    if float_age_filter==0:\n",
    "                        var = 'DOXY_ADJUSTED_offset_trimmed'\n",
    "                    else:\n",
    "                        var = 'DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa])\n",
    "                        age_title = str(float_age_bins[fa]) + ' to ' + str(float_age_bins[fa+1]) + 'days '\n",
    "\n",
    "\n",
    "\n",
    "                    nmean = np.around(dataset_2_filter[var].mean().values, decimals=1)\n",
    "                    ncount = np.around(dataset_2_filter[var].count().values, decimals=1)\n",
    "                    nstd = np.around(dataset_2_filter[var].std().values, decimals=1)\n",
    "                    nmedian = np.around(dataset_2_filter[var].median().values, decimals=1)\n",
    "\n",
    "                    percent_sat_mag = data_p/data_mag_p*100\n",
    "                    nmedian_percent_mag = np.around(percent_sat_mag.median(), decimals=1)\n",
    "\n",
    "                    # temp_var = dataset_2_filter.copy()\n",
    "                    # t_stat, p_value = stats.ttest_1samp(a=temp_var, popmean=0, nan_policy='omit') ############\n",
    "\n",
    "                    # CI_vals = stats.norm.interval(confidence=CI_level, \n",
    "                    #                 loc=np.nanmean(temp_var[~np.isnan(temp_var)].values), \n",
    "                    #                 scale = stats.sem(temp_var[~np.isnan(temp_var)].values))\n",
    "                    # CI_low = np.around(CI_vals[0], decimals=1)\n",
    "                    # CI_high = np.around(CI_vals[1], decimals=1)\n",
    "                    ax1 = plt.subplot(len(cal_groups_a),4,(1+4*idx))\n",
    "                    if float_age_filter==0:\n",
    "                        plt.title(cal_a.strip())\n",
    "                    else:\n",
    "                        plt.title(cal.strip() + ' ' + age_title)\n",
    "                    # plt.plot(data_p, pressure_p,'.')\n",
    "                    # plt.errorbar(nmean, pressure_p.mean(), xerr=nmean-CI_low, fmt='o', capsize=5,color='k')\n",
    "\n",
    "                    if np.sum(~np.isnan(dataset_2_filter[var].values))==0:\n",
    "                        continue\n",
    "                    plt.hist(dataset_2_filter[var], alpha=0.5, \n",
    "                             label=cal_a.strip() + ' ' + cal_b.strip() + ': \\nn: '+str(ncount) +  ' mean: ' + str(nmean) + ' $\\pm$' + str(nstd) + ' med.' + str(nmedian))\n",
    "\n",
    "                    plt.grid('on')\n",
    "                    ax2 = plt.subplot(len(cal_groups_a),4,(2+4*idx))\n",
    "                    plt.title('+/- 20 umol/kg')\n",
    "                    # plt.plot(data_p, pressure_p,'.')\n",
    "                    # plt.errorbar(nmean, pressure_p.mean(), xerr=nmean-CI_low, fmt='o', capsize=5,color='k')\n",
    "                    plt.hist(dataset_2_filter[var], alpha=0.5,\n",
    "                             label=cal_a.strip() + ' ' + cal_b.strip() + ': \\nn: '+str(ncount) +  ' mean: ' + str(nmean) + ' $\\pm$' + str(nstd)+ ' med.' + str(nmedian))\n",
    "\n",
    "                    # npres = np.around(pressure_p.mean(), decimals=1)\n",
    "                    plt.xlim([-20, 20])\n",
    "                    plt.grid('on')\n",
    "                    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "\n",
    "                    # o2_offset_data_table.append((idx, npres, ncount, nmean, nstd, p_value, CI_low, CI_high, nmedian, nmedian_percent_mag))\n",
    "\n",
    "                    # plot offset in percent saturation\n",
    "                    # ax2 = plt.subplot(len(cal_groups),4,(3+4*idx))\n",
    "                    # plt.title('Median percent Sat')\n",
    "                    # plt.plot(percent_sat_mag, pressure_p,'.')\n",
    "                    # plt.plot(nmedian_percent_mag, pressure_p.mean(),'sk', markersize=10, linewidth=10)\n",
    "\n",
    "                    # plt.errorbar(nmean, pressure_p.mean(), xerr=nmean-CI_low, fmt='o', capsize=5,color='k')\n",
    "\n",
    "                    # plt.xlim([-10, 10])\n",
    "                    # plt.grid('on')\n",
    "                \n",
    "            break\n",
    "        break\n",
    "    plot_filename = var + 'Offsets_grouped_by_' + parameter_a + ' ' + parameter_b\n",
    "    fig.savefig(grouped_plot_dir  + plot_filename + '.png', dpi=600)\n",
    "\n",
    "    break\n",
    "\n",
    "    #             dataset_p = dataset_n.to_dataframe()\n",
    "    #             offsets_o2_cal_group = dataset_p.groupby(parameter_a)\n",
    "    #             # offsets_pH = dataset_p.groupby([parameter_a, parameter_b])\n",
    "\n",
    "    #             if float_age_filter==0:\n",
    "    #                 var = 'DOXY_ADJUSTED_offset_trimmed'\n",
    "    #             else:\n",
    "    #                 var = 'DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa])\n",
    "    #                 age_title = str(float_age_bins[fa]) + ' to ' + str(float_age_bins[fa+1]) + 'days '\n",
    "\n",
    "    #             if np.logical_and(idx==0, parameter_a=='o2_calib_air_group'):\n",
    "    #                 # print(idx)\n",
    "    #                 # print(cal)\n",
    "    #                 data_p = dataset_n[var].copy()\n",
    "    #                 pressure_p = dataset_n.PRES_ADJUSTED_float\n",
    "    #                 data_mag_p = dataset_n['DOXY_ADJUSTED_float'].copy()\n",
    "    #                 if float_age_filter==1:\n",
    "    #                     # remove data with too few points if looking at float age - otherwise it already happened during the pressure level filtering\n",
    "    #                     count_p = dataset_n[var + '_count']\n",
    "    #                     data_p[count_p < 20] = np.nan\n",
    "\n",
    "    #             else:\n",
    "    #                 temp_dataset = offsets_o2_cal_group.get_group(cal)\n",
    "    #                 data_p = temp_dataset[var].copy()\n",
    "    #                 pressure_p = temp_dataset.PRES_ADJUSTED_float\n",
    "    #                 data_mag_p = temp_dataset['DOXY_ADJUSTED_float'].copy()\n",
    "\n",
    "    #                 if float_age_filter==1:\n",
    "    #                     # remove data with too few points if looking at float age - otherwise it already happened during the pressure level filtering\n",
    "    #                     count_p = temp_dataset[var + '_count']\n",
    "    #                     data_p[count_p < 20] = np.nan\n",
    "\n",
    "            \n",
    "\n",
    "    #             \n",
    "\n",
    "    # for idx, cal in enumerate(cal_groups):\n",
    "    #     plt.subplot(len(cal_groups),4,(1+4*idx))\n",
    "    #     plt.gca().invert_yaxis()\n",
    "    #     plt.plot([0,0], [0, 2000], 'k-')\n",
    "    #     plt.subplot(len(cal_groups),4,(2+4*idx))\n",
    "    #     plt.gca().invert_yaxis()\n",
    "    #     plt.plot([0,0], [0, 2000], 'k-')\n",
    "\n",
    "    #     plt.subplot(len(cal_groups),4,(3+4*idx))\n",
    "    #     plt.gca().invert_yaxis()\n",
    "    #     plt.plot([0,0], [0, 2000], 'k-')\n",
    "\n",
    "    # o2_offset_data_table = [row for row in o2_offset_data_table if not np.isnan(row[1])]\n",
    "    # sorted_data_table = sorted(o2_offset_data_table, key=lambda x: x[0])\n",
    "\n",
    "    # # base_value = 1\n",
    "    # # increment = 3\n",
    "    # # num_iterations = 10  \n",
    "\n",
    "    # # for idx in range(num_iterations):\n",
    "    # #     current_value = base_value + increment * idx\n",
    "    # #     print(f\"Index: {idx}, Value: {current_value}\")\n",
    "\n",
    "    # for idx, cal in enumerate(cal_groups):\n",
    "    #     filtered_data_table = [row for row in sorted_data_table if row[0] == idx]\n",
    "    #     # Create a string buffer to store the table\n",
    "    #     table_buffer = StringIO()\n",
    "\n",
    "    #     # Convert the o2_offset_data_table to a table and write it to the buffer\n",
    "    #     print(tabulate(filtered_data_table, \n",
    "    #                 headers=['Cal group', 'Pressure', 'count',\n",
    "    #                             'mean', 'std', 'p_value', \n",
    "    #                             str(CI_level*100) + '% CI low', str(CI_level*100) + '% CI high', \n",
    "    #                             'median', 'med_per'], tablefmt='grid'), file=table_buffer)\n",
    "\n",
    "    #     # Display the table in a subplot\n",
    "    #     plt.subplot(len(cal_groups),4,(4+4*idx))\n",
    "    #     plt.title(cal.strip())\n",
    "\n",
    "    #     plt.axis('off')  # Turn off axis for the table\n",
    "    #     # plt.text(0.01, 0.01, table_buffer.getvalue(), va='top', ha='left', fontfamily='monospace', fontsize=8)\n",
    "    #     plt.text(0.01, 0.95, table_buffer.getvalue(), va='top', ha='left', transform=plt.gca().transAxes, fontfamily='monospace', fontsize=8)\n",
    "        \n",
    "    #     # break\n",
    "\n",
    "    # plot_filename = var + '_vs_depth_grouped_' + parameter_a + '_year_filt_' + str(year_filt) +'_' + str(year_plus_minus)  + '_abs_Lat_cut_' + str(abs_lat_cutoff_shallow)\n",
    "    # plt.savefig(grouped_plot_dir + plot_filename + '.png', dpi=300)\n",
    "\n",
    "    # o2_offset_table_cal_groups = pd.DataFrame(columns=['Cal group','pressure','offset', 'CI_low', 'CI_high', 'n_count', 'per_med'])\n",
    "    \n",
    "    # for idx, cal in enumerate(cal_groups):\n",
    "    #     filtered_data_table = [row for row in sorted_data_table if row[0] == idx]\n",
    "\n",
    "    #     for i in range(0,len(filtered_data_table)):\n",
    "    #         new_data_o2_direct_impact = [(cal.strip(), \n",
    "    #                                     filtered_data_table[i][1], \n",
    "    #                                     filtered_data_table[i][3],\n",
    "    #                                     filtered_data_table[i][6],\n",
    "    #                                     filtered_data_table[i][7],\n",
    "    #                                     filtered_data_table[i][2],\n",
    "    #                                     filtered_data_table[i][9])]\n",
    "    #         new_data_o2_direct_impact_df = pd.DataFrame(new_data_o2_direct_impact, columns=['Cal group','pressure','offset', 'CI_low', 'CI_high', 'n_count', 'per_med'])\n",
    "    #         o2_offset_table_cal_groups = pd.concat([o2_offset_table_cal_groups, new_data_o2_direct_impact_df], ignore_index=True)\n",
    "\n",
    "    # if parameter_a=='project_name':\n",
    "    #     n_cutoff= 10\n",
    "    #     x_lim = [-25, 10]\n",
    "    # elif parameter_a=='DOXY_sensor':\n",
    "    #     n_cutoff = 5\n",
    "    #     x_lim = [-15, 5]\n",
    "    # elif parameter_a == 'o2_calib_air_group':\n",
    "    #     n_cutoff = 0\n",
    "    #     x_lim = [-10, 2.5]\n",
    "    # elif parameter_a == 'data_centre':\n",
    "    #     n_cutoff = 5\n",
    "    #     x_lim = [-22, 5]\n",
    "\n",
    "\n",
    "    # fig = plt.figure(figsize=(12,8))\n",
    "    # ax1 = plt.subplot(1,2,1)\n",
    "    # ax2 = plt.subplot(1,2,2)\n",
    "\n",
    "    # for cal in cal_groups:\n",
    "    #     temp_group = o2_offset_table_cal_groups[o2_offset_table_cal_groups['Cal group']==cal.strip()]\n",
    "\n",
    "    #     if temp_group.n_count.values.mean()<n_cutoff:\n",
    "    #         continue\n",
    "\n",
    "    #     line, = ax1.plot(temp_group.offset, temp_group.pressure, label=cal.strip() + ' (' + str(temp_group.n_count.values.mean()) + ')')\n",
    "    #     line, = ax2.plot(temp_group.per_med, temp_group.pressure, label=cal.strip() + ' (' + str(temp_group.n_count.values.mean()) + ')')\n",
    "\n",
    "    #     line_color = line.get_color()\n",
    "    #     for i in range(0,len(temp_group)):\n",
    "    #         ax1.errorbar(temp_group.offset.values[i], temp_group.pressure.values[i], xerr=temp_group.offset.values[i]-temp_group.CI_low.values[i],\n",
    "    #                     fmt='o', capsize=5, color=line_color)\n",
    "\n",
    "    # ax1.invert_yaxis()\n",
    "    # ax2.invert_yaxis()\n",
    "\n",
    "    # ax1.plot([0,0], [0, 2000], 'k-')\n",
    "    # ax2.plot([0,0], [0, 2000], 'k-')\n",
    "\n",
    "    # ax1.grid('on')\n",
    "    # ax2.grid('on')\n",
    "\n",
    "    # ax1.set_xlim(x_lim)\n",
    "    # ax1.legend()\n",
    "    # if float_age_filter==0:\n",
    "    #     ax1.set_title(parameter_a + ' cutoff of ' + str(n_cutoff) + ' floats' + ' year_filt=' + str(year_filt))\n",
    "\n",
    "    # else:\n",
    "    #     ax1.set_title(parameter_a + ' cutoff of ' + str(n_cutoff) + ' floats' + ' year_filt=' + str(year_filt) + ' ' + age_title)\n",
    "    # ax2.set_title('median_percent')\n",
    "\n",
    "    # ax1.set_xlabel('umol/kg')\n",
    "    # ax2.set_xlabel('%')\n",
    "\n",
    "    # ax1.set_ylabel('Pressure')\n",
    "    # ax2.set_ylabel('Pressure')\n",
    "\n",
    "    # plot_filename = var + 'Offsets_vs_depth_grouped_' + parameter_a + '_year_filt' + str(year_filt) +'_' + str(year_plus_minus) + '_all_cutoff_' + str(n_cutoff)  + '_abs_Lat_cut_' + str(abs_lat_cutoff_shallow)\n",
    "    # fig.savefig(grouped_plot_dir  + plot_filename + '.png', dpi=600)\n",
    "\n",
    "    \n",
    "    # if float_age_filter==0:\n",
    "    #     break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots of means, SDs, and medians for various groupings \n",
    "cal_groups_all  = [o2_sensors, big_projects, data_centers]\n",
    "parameter_a_all= ['DOXY_sensor', 'project_name', 'data_centre']\n",
    "\n",
    "# cal_groups = big_projects\n",
    "# parameter_a = 'project_name'\n",
    "\n",
    "# cal_groups = data_centers\n",
    "# parameter_a = 'data_centre'\n",
    "for n, cal_groups in enumerate(cal_groups_all):\n",
    "    parameter_a = parameter_a_all[n]\n",
    "    cal_group_labels = []\n",
    "    for fa in range(len(float_age_bins)-1):\n",
    "        o2_offset_data_table = []\n",
    "        means = []\n",
    "        stds = []\n",
    "        medians = []\n",
    "        counts = []\n",
    "\n",
    "        fig = plt.figure(figsize=(10,8))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "        for filename in glodap_offsets_filenames:\n",
    "            \n",
    "            for j in range(len(pressure_levels)-1):\n",
    "                try:\n",
    "                    dataset_n = mean_gdap_offsets[filename][f'level_{pressure_levels[j]}']\n",
    "                except:\n",
    "                    continue    \n",
    "                for idx, cal in enumerate(cal_groups):\n",
    "                    dataset_p = dataset_n.to_dataframe()\n",
    "                    offsets_o2_cal_group = dataset_p.groupby(parameter_a)\n",
    "                    if float_age_filter==0:\n",
    "                        var = 'DOXY_ADJUSTED_offset_trimmed'\n",
    "                    else:\n",
    "                        var = 'DOXY_ADJUSTED_offset_' + 'age_' + str(float_age_bins[fa])\n",
    "\n",
    "                    if np.logical_and(idx==0, parameter_a=='o2_calib_air_group'):\n",
    "                        # print(idx)\n",
    "                        # print(cal)\n",
    "                        data_p = dataset_n[var].copy()\n",
    "                        pressure_p = dataset_n.PRES_ADJUSTED_float\n",
    "\n",
    "                        if float_age_filter==1:\n",
    "                            # remove data with too few points if looking at float age - otherwise it already happened during the pressure level filtering\n",
    "                            count_p = dataset_n[var + '_count']\n",
    "                            data_p[count_p < 20] = np.nan\n",
    "                    else:\n",
    "                        temp_dataset = offsets_o2_cal_group.get_group(cal)\n",
    "                        data_p = temp_dataset[var].copy()\n",
    "                        pressure_p = temp_dataset.PRES_ADJUSTED_float\n",
    "\n",
    "                        if float_age_filter==1:\n",
    "                            # remove data with too few points if looking at float age - otherwise it already happened during the pressure level filtering\n",
    "                            count_p = temp_dataset[var + '_count']\n",
    "                            data_p[count_p < 20] = np.nan\n",
    "                        print(cal)\n",
    "                    # nmean = np.around(data_p.mean(), decimals=1)\n",
    "                    # ncount = np.around(data_p.count(), decimals=1)\n",
    "                    # nstd = np.around(data_p.std(), decimals=1)\n",
    "                    # nmedian = np.around(data_p.median(), decimals=1)\n",
    "                    cal_group_labels.append(cal.strip() + ' (' + str(data_p.count())+ ')')\n",
    "\n",
    "                    means.append(data_p.mean())\n",
    "                    stds.append(data_p.std())\n",
    "                    counts.append(data_p.count())\n",
    "                    medians.append(data_p.median())\n",
    "\n",
    "        x = np.arange(len(cal_group_labels))\n",
    "\n",
    "        plt.hlines(0, xmin=0, xmax=len(cal_group_labels)-1, colors='k')\n",
    "        plt.grid('on')\n",
    "        # plt.plot(x, means, 's', label='mean')\n",
    "        plt.errorbar(x, means, yerr=stds, marker='s', label='mean $\\pm$ 1 SD', capsize=4)\n",
    "        plt.plot(x, medians, 'x', label='median', zorder=4, linewidth=4)\n",
    "\n",
    "        ax.set_xticks(x, labels=cal_group_labels, rotation=45, ha='right')\n",
    "        # ax.set_xticklabels(cal_group_labels, rotation=45, ha='right')\n",
    "        ax.set_title(parameter_a)\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plot_filename = var + '_means_' + parameter_a + '_year_filt_' + str(year_filt) +'_' + str(year_plus_minus)\n",
    "\n",
    "        plt.savefig(grouped_plot_dir + plot_filename + '.png')\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "o2_offset_table_cal_groups = pd.DataFrame(columns=['Cal group','pressure','offset', 'CI_low', 'CI_high', 'n_count'])\n",
    "for idx, cal in enumerate(cal_groups):\n",
    "    filtered_data_table = [row for row in sorted_data_table if row[0] == idx]\n",
    "\n",
    "    for i in range(0,len(filtered_data_table)):\n",
    "        new_data_o2_direct_impact = [(cal.strip(), \n",
    "                                      filtered_data_table[i][1], \n",
    "                                      filtered_data_table[i][3],\n",
    "                                      filtered_data_table[i][6],\n",
    "                                      filtered_data_table[i][7],\n",
    "                                      filtered_data_table[i][2])]\n",
    "        new_data_o2_direct_impact_df = pd.DataFrame(new_data_o2_direct_impact, columns=['Cal group','pressure','offset', 'CI_low', 'CI_high', 'n_count'])\n",
    "        o2_offset_table_cal_groups = pd.concat([o2_offset_table_cal_groups, new_data_o2_direct_impact_df], ignore_index=True)\n",
    "\n",
    "if parameter_a=='project_name':\n",
    "    n_cutoff= 30\n",
    "    x_lim = [-25, 10]\n",
    "elif parameter_a=='DOXY_sensor':\n",
    "    n_cutoff = 20\n",
    "    x_lim = [-15, 5]\n",
    "elif parameter_a == 'o2_calib_air_group':\n",
    "    n_cutoff = 0\n",
    "    x_lim = [-10, 2.5]\n",
    "elif parameter_a == 'data_centre':\n",
    "    n_cutoff = 30\n",
    "    x_lim = [-22, 5]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "\n",
    "for cal in cal_groups:\n",
    "    temp_group = o2_offset_table_cal_groups[o2_offset_table_cal_groups['Cal group']==cal.strip()]\n",
    "\n",
    "    if temp_group.n_count.values.mean()<n_cutoff:\n",
    "        continue\n",
    "\n",
    "    line, = plt.plot(temp_group.offset, temp_group.pressure, label=cal.strip())\n",
    "    \n",
    "    line_color = line.get_color()\n",
    "    for i in range(0,len(temp_group)):\n",
    "        plt.errorbar(temp_group.offset.values[i], temp_group.pressure.values[i], xerr=temp_group.offset.values[i]-temp_group.CI_low.values[i],\n",
    "                      fmt='o', capsize=5, color=line_color)\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.plot([0,0], [0, 2000], 'k-')\n",
    "plt.grid('on')\n",
    "plt.xlim(x_lim)\n",
    "plt.legend()\n",
    "plt.title(parameter_a + ' cutoff of ' + str(n_cutoff) + ' floats' + ' year_filt=' + str(year_filt))\n",
    "plt.xlabel('umol/kg')\n",
    "plt.ylabel('Pressure')\n",
    "plot_filename = 'Offsets_vs_depth_grouped_' + parameter_a + '_year_filt' + str(year_filt) +'_' + str(year_plus_minus) + '_all_cutoff_' + str(n_cutoff)\n",
    "plt.savefig(grouped_plot_dir  + plot_filename + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdap_offsets[filename]['level_1500'].main_float_wmo=='5903615'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with nan values in the first element\n",
    "o2_offset_data_table = [row for row in o2_offset_data_table if not np.isnan(row[1])]\n",
    "sorted_data_table = sorted(o2_offset_data_table, key=lambda x: x[0])\n",
    "\n",
    "# base_value = 1\n",
    "# increment = 3\n",
    "# num_iterations = 10  # You can adjust this based on the number of iterations you need\n",
    "\n",
    "# for idx in range(num_iterations):\n",
    "#     current_value = base_value + increment * idx\n",
    "#     print(f\"Index: {idx}, Value: {current_value}\")\n",
    "\n",
    "for idx, cal in enumerate(cal_groups):\n",
    "    filtered_data_table = [row for row in sorted_data_table if row[0] == idx]\n",
    "    # print(cal)\n",
    "    # print(tabulate(filtered_data_table, headers=['Cal group', 'Pressure', 'count', 'mean', 'std', 'p_value', str(CI_level*100) + '% CI low', str(CI_level*100) + '% CI high', 'median', 'min', 'max'], tablefmt='grid'))\n",
    "    # Create a string buffer to store the table\n",
    "    table_buffer = StringIO()\n",
    "\n",
    "    # Convert the o2_offset_data_table to a table and write it to the buffer\n",
    "    print(tabulate(filtered_data_table, headers=['Cal group', 'Pressure', 'count', 'mean', 'std', 'p_value', str(CI_level*100) + '% CI low', str(CI_level*100) + '% CI high', 'median'], tablefmt='grid'), file=table_buffer)\n",
    "\n",
    "    # Display the table in a subplot\n",
    "    plt.subplot(len(cal_groups),3,(1+3*idx))\n",
    "    plt.axis('off')  # Turn off axis for the table\n",
    "    plt.text(0.1, 0.1, table_buffer.getvalue(), va='top', ha='left', fontfamily='monospace', fontsize=8)\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "float_oxygen_offset_impacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
