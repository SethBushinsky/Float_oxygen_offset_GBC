{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad569a9",
   "metadata": {},
   "source": [
    "### Download sprof and meta files from GDACs\n",
    "\n",
    "Code modified from GO-BGC python tutorial: https://github.com/go-bgc/workshop-python/blob/main/GO_BGC_Workshop_Python_tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0616515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import urllib3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1985da17",
   "metadata": {},
   "source": [
    "## Set directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717ea56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a user-created text file to point to local directories to avoid having to change this every time \n",
    "# we update code\n",
    "lines=[]\n",
    "with open('path_file.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "count = 0\n",
    "for line in lines:\n",
    "    count += 1\n",
    "    index = line.find(\"=\")\n",
    "    #print(f'line {count}: {line}')\n",
    "    #print(index)\n",
    "    #print(line[0:index])\n",
    "    line = line.rstrip()\n",
    "    if line[0:index].find(\"argo\")>=0:\n",
    "        root=line[index+1:]\n",
    "    elif line[0:index].find(\"liar\")>=0:\n",
    "        liar_dir=line[index+1:]\n",
    "    elif line[0:index].find(\"matlab\")>=0:\n",
    "        matlab_dir=line[index+1:]\n",
    "profile_dir = root\n",
    "root = root + '../'\n",
    "\n",
    "#make profile_dir if it doesn't exist\n",
    "if not os.path.isdir(profile_dir):\n",
    "    os.mkdir(profile_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64438d40",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c994854",
   "metadata": {},
   "source": [
    "### Function to download a single file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5221429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url_path,filename,save_to=None,overwrite=False,verbose=True):\n",
    "    \"\"\" Downloads and saves a file from a given URL using HTTP protocol.\n",
    "\n",
    "    Note: If '404 file not found' error returned, function will return without downloading anything.\n",
    "    \n",
    "    Arguments:\n",
    "        url_path: root URL to download from including trailing slash ('/')\n",
    "        filename: filename to download including suffix\n",
    "        save_to: None (to download to root Google Drive GO-BGC directory)\n",
    "                 or directory path\n",
    "        overwrite: False to leave existing files in place\n",
    "                   or True to overwrite existing files\n",
    "        verbose: True to announce progress\n",
    "                 or False to stay silent\n",
    "    \n",
    "    \"\"\"\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    if save_to is None:\n",
    "        save_to = root\n",
    "\n",
    "    try:\n",
    "        if filename in os.listdir(save_to):\n",
    "            if not overwrite:\n",
    "                if verbose: print('>>> File ' + filename + ' already exists. Leaving current version.')\n",
    "                return\n",
    "            else:\n",
    "                if verbose: print('>>> File ' + filename + ' already exists. Overwriting with new version.')\n",
    "\n",
    "        def get_func(url,stream=True):\n",
    "            try:\n",
    "                return requests.get(url,stream=stream,auth=None,verify=False)\n",
    "            except requests.exceptions.ConnectionError as error_tag:\n",
    "                print('Error connecting:',error_tag)\n",
    "                time.sleep(1)\n",
    "                return get_func(url,stream=stream)\n",
    "\n",
    "        response = get_func(url_path + filename,stream=True)\n",
    "\n",
    "        if response.status_code == 404:\n",
    "            if verbose: print('>>> File ' + filename + ' returned 404 error during download.')\n",
    "            return\n",
    "        with open(save_to + filename,'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw,out_file)\n",
    "        del response\n",
    "        if verbose: \n",
    "            print('>>> Successfully downloaded ' + filename + '.')\n",
    "\n",
    "    except:\n",
    "        if verbose: \n",
    "            print('>>> An error occurred while trying to download ' + filename + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceddd34b",
   "metadata": {},
   "source": [
    "### Function to download and parse GDAC synthetic profile index file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "922044ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argo_gdac(lat_range=None,lon_range=None,start_date=None,end_date=None,sensors=None,floats=None,\n",
    "              overwrite_index=False,overwrite_profiles=False,skip_download=False,\n",
    "              download_individual_profs=False,download_meta=False,save_to=None,verbose=True):\n",
    "    \"\"\" Downloads GDAC Sprof index file, then selects float profiles based on criteria.\n",
    "      Either returns information on profiles and floats (if skip_download=True) or downloads them (if False).\n",
    "\n",
    "      Arguments:\n",
    "          lat_range: None, to select all latitudes\n",
    "                     or [lower, upper] within -90 to 90 (selection is inclusive)\n",
    "          lon_range: None, to select all longitudes\n",
    "                     or [lower, upper] within either -180 to 180 or 0 to 360 (selection is inclusive)\n",
    "                     NOTE: longitude range is allowed to cross -180/180 or 0/360\n",
    "          start_date: None or datetime object\n",
    "          end_date:   None or datetime object\n",
    "          sensors: None, to select profiles with any combination of sensors\n",
    "                   or string or list of strings to specify required sensors\n",
    "                   > note that common options include PRES, TEMP, PSAL, DOXY, CHLA, BBP700,\n",
    "                                                      PH_IN_SITU_TOTAL, and NITRATE\n",
    "          floats: None, to select any floats matching other criteria\n",
    "                  or int or list of ints specifying floats' WMOID numbers\n",
    "          overwrite_index: False to keep existing downloaded GDAC index file, or True to download new index\n",
    "          overwrite_profiles: False to keep existing downloaded profile files, or True to download new files\n",
    "          skip_download: True to skip download and return: (<list of WMOIDs>, <DataFrame of index file subset>,\n",
    "                                                            <list of downloaded filenames [if applicable]>)\n",
    "                         or False to download those profiles\n",
    "          download_individual_profs: False to download single Sprof file containing all profiles for each float\n",
    "                                     or True to download individual profile files for each float\n",
    "          save_to: None to download to Google Drive \"/GO-BGC Workshop/Profiles\" directory\n",
    "                   or string to specify directory path for profile downloads\n",
    "          verbose: True to announce progress, or False to stay silent\n",
    "\n",
    "    \"\"\"\n",
    "    # Paths\n",
    "    url_root = 'https://www.usgodae.org/ftp/outgoing/argo/'\n",
    "    dac_url_root = url_root + 'dac/'\n",
    "    index_filename = 'argo_synthetic-profile_index.txt'\n",
    "    if save_to is None: save_to = root\n",
    "\n",
    "    # Download GDAC synthetic profile index file\n",
    "    download_file(url_root,index_filename,overwrite=overwrite_index)\n",
    "\n",
    "    # Load index file into Pandas DataFrame\n",
    "    gdac_index = pd.read_csv(root + index_filename,delimiter=',',header=8,parse_dates=['date','date_update'],\n",
    "                          date_parser=lambda x: pd.to_datetime(x,format='%Y%m%d%H%M%S'))\n",
    "\n",
    "    # Establish time and space criteria\n",
    "    if lat_range is None:  lat_range = [-90.0,90.0]\n",
    "    if lon_range is None:  lon_range = [-180.0,180.0]\n",
    "    elif lon_range[0] > 180 or lon_range[1] > 180:\n",
    "        if lon_range[0] > 180: lon_range[0] -= 360\n",
    "        if lon_range[1] > 180: lon_range[1] -= 360\n",
    "    if start_date is None: start_date = datetime(1900,1,1)\n",
    "    if end_date is None:   end_date = datetime(2200,1,1)\n",
    "\n",
    "    float_wmoid_regexp = r'[a-z]*/[0-9]*/profiles/[A-Z]*([0-9]*)_[0-9]*[A-Z]*.nc'\n",
    "    gdac_index['wmoid'] = gdac_index['file'].str.extract(float_wmoid_regexp).astype(int)\n",
    "    filepath_main_regexp = '([a-z]*/[0-9]*/)profiles/[A-Z]*[0-9]*_[0-9]*[A-Z]*.nc'\n",
    "    gdac_index['filepath_main'] = gdac_index['file'].str.extract(filepath_main_regexp)\n",
    "    filepath_regexp = '([a-z]*/[0-9]*/profiles/)[A-Z]*[0-9]*_[0-9]*[A-Z]*.nc'\n",
    "    gdac_index['filepath'] = gdac_index['file'].str.extract(filepath_regexp)\n",
    "    filename_regexp = '[a-z]*/[0-9]*/profiles/([A-Z]*[0-9]*_[0-9]*[A-Z]*.nc)'\n",
    "    gdac_index['filename'] = gdac_index['file'].str.extract(filename_regexp)\n",
    "\n",
    "    # Subset profiles based on time and space criteria\n",
    "    gdac_index_subset = gdac_index.loc[np.logical_and.reduce([gdac_index['latitude'] >= lat_range[0],\n",
    "                                                            gdac_index['latitude'] <= lat_range[1],\n",
    "                                                            gdac_index['date'] >= start_date,\n",
    "                                                            gdac_index['date'] <= end_date]),:]\n",
    "    if lon_range[1] >= lon_range[0]:    # range does not cross -180/180 or 0/360\n",
    "        gdac_index_subset = gdac_index_subset.loc[np.logical_and(gdac_index_subset['longitude'] >= lon_range[0],\n",
    "                                                             gdac_index_subset['longitude'] <= lon_range[1])]\n",
    "    elif lon_range[1] < lon_range[0]:   # range crosses -180/180 or 0/360\n",
    "        gdac_index_subset = gdac_index_subset.loc[np.logical_or(gdac_index_subset['longitude'] >= lon_range[0],\n",
    "                                                            gdac_index_subset['longitude'] <= lon_range[1])]\n",
    "\n",
    "    # If requested, subset profiles using float WMOID criteria\n",
    "    if floats is not None:\n",
    "        if type(floats) is not list: floats = [floats]\n",
    "        gdac_index_subset = gdac_index_subset.loc[gdac_index_subset['wmoid'].isin(floats),:]\n",
    "\n",
    "    # If requested, subset profiles using sensor criteria\n",
    "    if sensors is not None:\n",
    "        if type(sensors) is not list: sensors = [sensors]\n",
    "        for sensor in sensors:\n",
    "            gdac_index_subset = gdac_index_subset.loc[gdac_index_subset['parameters'].str.contains(sensor),:]\n",
    "\n",
    "    # Examine subsetted profiles\n",
    "    wmoids = gdac_index_subset['wmoid'].unique()\n",
    "    wmoid_filepaths = gdac_index_subset['filepath_main'].unique()\n",
    "\n",
    "    # Just return list of floats and DataFrame with subset of index file, or download each profile\n",
    "    if not skip_download:\n",
    "        downloaded_filenames = []\n",
    "        if download_individual_profs:\n",
    "            for p_idx in gdac_index_subset.index:\n",
    "                download_file(dac_url_root + gdac_index_subset.loc[p_idx]['filepath'],\n",
    "                      gdac_index_subset.loc[p_idx]['filename'],\n",
    "                      save_to=save_to,overwrite=overwrite_profiles,verbose=verbose)\n",
    "                downloaded_filenames.append(gdac_index_subset.loc[p_idx]['filename'])\n",
    "        else:\n",
    "            for f_idx, wmoid_filepath in enumerate(wmoid_filepaths):\n",
    "                download_file(dac_url_root + wmoid_filepath,str(wmoids[f_idx]) + '_Sprof.nc',\n",
    "                      save_to=save_to,overwrite=overwrite_profiles,verbose=verbose)\n",
    "                downloaded_filenames.append(str(wmoids[f_idx]) + '_Sprof.nc')\n",
    "                if download_meta:\n",
    "                    download_file(dac_url_root + wmoid_filepath,str(wmoids[f_idx]) + '_meta.nc',\n",
    "                      save_to=save_to,overwrite=overwrite_profiles,verbose=verbose)\n",
    "        return wmoids, gdac_index_subset, downloaded_filenames\n",
    "    else:\n",
    "        return wmoids, gdac_index_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a46dcc",
   "metadata": {},
   "source": [
    "## Run these three cells to download all floats with DOXY, PH, or NITRATE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads argo_synthetic_profile_index.txt to data folder. This file has a list of all synthetic profiles, which should capture all bgc argo data\n",
    "#get wmo indices of BGC floats (DOXY, PH_IN_SITU_TOTAL, NITRATE)\n",
    "wmoids_doxy, gdac_index = argo_gdac(sensors='DOXY',floats=None,\n",
    "                               overwrite_index=True,\n",
    "                               skip_download=True)\n",
    "wmoids_ph, gdac_index = argo_gdac(sensors='PH_IN_SITU_TOTAL',floats=None,\n",
    "                               skip_download=True)\n",
    "wmoids_nitrate, gdac_index = argo_gdac(sensors='NITRATE',floats=None,\n",
    "                               skip_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac8542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine wmoids to one unique list\n",
    "wmoids = np.concatenate((wmoids_doxy,wmoids_ph,wmoids_nitrate))\n",
    "wmoids_all = np.unique(wmoids)\n",
    "wmoids_all = wmoids_all[~np.isnan(wmoids_all)].tolist()\n",
    "print(len(wmoids_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e99a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#re-run argo_gdac with full list and download Sprof and meta files\n",
    "wmoids_bgc, gdac_index, downloaded_filenames = argo_gdac(floats=wmoids_all,\n",
    "                               skip_download=False, download_meta=True,save_to=profile_dir)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "float_oxygen_offset_impacts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
